{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2e8825-b1d2-4c6d-81ee-75c05f88d00c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Traffic Crash Analysis\n",
    "\n",
    "### Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c17149e-687f-4fda-b847-91bd4548eba6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting sodapy\n  Downloading sodapy-2.2.0-py2.py3-none-any.whl (15 kB)\nCollecting requests>=2.28.1\n  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.28.1->sodapy) (3.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.28.1->sodapy) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.28.1->sodapy) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.28.1->sodapy) (2021.10.8)\nInstalling collected packages: requests, sodapy\n  Attempting uninstall: requests\n    Found existing installation: requests 2.27.1\n    Not uninstalling requests at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-1c4fed61-24d6-4e48-a8ab-e137e5b3bfe9\n    Can't uninstall 'requests'. No files were found to uninstall.\nSuccessfully installed requests-2.31.0 sodapy-2.2.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install sodapy\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3229176b-3990-44da-9583-71919c5cd60e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2915852555945579>:11\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Specify the columns you want to retrieve\u001B[39;00m\n",
       "\u001B[1;32m      9\u001B[0m desired_columns \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcrash_record_id,crash_date,crash_type,num_units,weather_condition,most_severe_injury,latitude,longitude\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 11\u001B[0m results \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m85ca-t3if\u001B[39m\u001B[38;5;124m\"\u001B[39m, select\u001B[38;5;241m=\u001B[39mdesired_columns, limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m800000\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m60\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-1c4fed61-24d6-4e48-a8ab-e137e5b3bfe9/lib/python3.9/site-packages/sodapy/socrata.py:412\u001B[0m, in \u001B[0;36mSocrata.get\u001B[0;34m(self, dataset_identifier, content_type, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    409\u001B[0m params\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n",
       "\u001B[1;32m    410\u001B[0m params \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mclear_empty_values(params)\n",
       "\u001B[0;32m--> 412\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_perform_request\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    413\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\n",
       "\u001B[1;32m    414\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-1c4fed61-24d6-4e48-a8ab-e137e5b3bfe9/lib/python3.9/site-packages/sodapy/socrata.py:555\u001B[0m, in \u001B[0;36mSocrata._perform_request\u001B[0;34m(self, request_type, resource, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    553\u001B[0m \u001B[38;5;66;03m# handle errors\u001B[39;00m\n",
       "\u001B[1;32m    554\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m200\u001B[39m, \u001B[38;5;241m202\u001B[39m):\n",
       "\u001B[0;32m--> 555\u001B[0m     \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    557\u001B[0m \u001B[38;5;66;03m# when responses have no content body (ie. delete, set_permission),\u001B[39;00m\n",
       "\u001B[1;32m    558\u001B[0m \u001B[38;5;66;03m# simply return the whole response\u001B[39;00m\n",
       "\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m response\u001B[38;5;241m.\u001B[39mtext:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-1c4fed61-24d6-4e48-a8ab-e137e5b3bfe9/lib/python3.9/site-packages/sodapy/utils.py:30\u001B[0m, in \u001B[0;36mraise_for_status\u001B[0;34m(response)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m more_info \u001B[38;5;129;01mand\u001B[39;00m more_info\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m!=\u001B[39m response\u001B[38;5;241m.\u001B[39mreason\u001B[38;5;241m.\u001B[39mlower():\n",
       "\u001B[1;32m     29\u001B[0m     http_error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(more_info)\n",
       "\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mHTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39mresponse)\n",
       "\n",
       "\u001B[0;31mHTTPError\u001B[0m: 400 Client Error: Bad Request.\n",
       "\tUnrecognized arguments [timeout]"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2915852555945579>:11\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Specify the columns you want to retrieve\u001B[39;00m\n\u001B[1;32m      9\u001B[0m desired_columns \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcrash_record_id,crash_date,crash_type,num_units,weather_condition,most_severe_injury,latitude,longitude\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 11\u001B[0m results \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m85ca-t3if\u001B[39m\u001B[38;5;124m\"\u001B[39m, select\u001B[38;5;241m=\u001B[39mdesired_columns, limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m800000\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m60\u001B[39m)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-1c4fed61-24d6-4e48-a8ab-e137e5b3bfe9/lib/python3.9/site-packages/sodapy/socrata.py:412\u001B[0m, in \u001B[0;36mSocrata.get\u001B[0;34m(self, dataset_identifier, content_type, **kwargs)\u001B[0m\n\u001B[1;32m    409\u001B[0m params\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n\u001B[1;32m    410\u001B[0m params \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mclear_empty_values(params)\n\u001B[0;32m--> 412\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_perform_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-1c4fed61-24d6-4e48-a8ab-e137e5b3bfe9/lib/python3.9/site-packages/sodapy/socrata.py:555\u001B[0m, in \u001B[0;36mSocrata._perform_request\u001B[0;34m(self, request_type, resource, **kwargs)\u001B[0m\n\u001B[1;32m    553\u001B[0m \u001B[38;5;66;03m# handle errors\u001B[39;00m\n\u001B[1;32m    554\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m200\u001B[39m, \u001B[38;5;241m202\u001B[39m):\n\u001B[0;32m--> 555\u001B[0m     \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[38;5;66;03m# when responses have no content body (ie. delete, set_permission),\u001B[39;00m\n\u001B[1;32m    558\u001B[0m \u001B[38;5;66;03m# simply return the whole response\u001B[39;00m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m response\u001B[38;5;241m.\u001B[39mtext:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-1c4fed61-24d6-4e48-a8ab-e137e5b3bfe9/lib/python3.9/site-packages/sodapy/utils.py:30\u001B[0m, in \u001B[0;36mraise_for_status\u001B[0;34m(response)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m more_info \u001B[38;5;129;01mand\u001B[39;00m more_info\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m!=\u001B[39m response\u001B[38;5;241m.\u001B[39mreason\u001B[38;5;241m.\u001B[39mlower():\n\u001B[1;32m     29\u001B[0m     http_error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(more_info)\n\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mHTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39mresponse)\n\n\u001B[0;31mHTTPError\u001B[0m: 400 Client Error: Bad Request.\n\tUnrecognized arguments [timeout]",
       "errorSummary": "<span class='ansi-red-fg'>HTTPError</span>: 400 Client Error: Bad Request.\n\tUnrecognized arguments [timeout]",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofchicago.org\", None)\n",
    "\n",
    "# Specify the columns you want to retrieve\n",
    "desired_columns = \"crash_record_id,crash_date,crash_type,num_units,weather_condition,most_severe_injury,latitude,longitude\"\n",
    "\n",
    "results = client.get(\"85ca-t3if\", select=desired_columns, limit=800000, timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb8512f-928f-45de-8a07-38be8594f2e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784815\n+--------------------+----------------+--------------------+------------------+----------------------+--------------------+-----------------+------------------+--------------------+--------------------+--------+------------------+--------------------+-----------+-----------+--------------------+----------------------+------------------+-------------+-------------+--------------------+-----------------------+----------------------+---------+----------------+-----------+------------------+--------------+------------------+---------+-----------+--------------+-----------------+---------+--------------------+--------------+--------------+-----------------------+---------------------------+-----------------------------+----------------------+----------------+----------+-----------------+-----------+------------+-------------+--------------------+\n|     CRASH_RECORD_ID|CRASH_DATE_EST_I|          CRASH_DATE|POSTED_SPEED_LIMIT|TRAFFIC_CONTROL_DEVICE|    DEVICE_CONDITION|WEATHER_CONDITION|LIGHTING_CONDITION|    FIRST_CRASH_TYPE|     TRAFFICWAY_TYPE|LANE_CNT|         ALIGNMENT|ROADWAY_SURFACE_COND|ROAD_DEFECT|REPORT_TYPE|          CRASH_TYPE|INTERSECTION_RELATED_I|NOT_RIGHT_OF_WAY_I|HIT_AND_RUN_I|       DAMAGE|DATE_POLICE_NOTIFIED|PRIM_CONTRIBUTORY_CAUSE|SEC_CONTRIBUTORY_CAUSE|STREET_NO|STREET_DIRECTION|STREET_NAME|BEAT_OF_OCCURRENCE|PHOTOS_TAKEN_I|STATEMENTS_TAKEN_I|DOORING_I|WORK_ZONE_I|WORK_ZONE_TYPE|WORKERS_PRESENT_I|NUM_UNITS|  MOST_SEVERE_INJURY|INJURIES_TOTAL|INJURIES_FATAL|INJURIES_INCAPACITATING|INJURIES_NON_INCAPACITATING|INJURIES_REPORTED_NOT_EVIDENT|INJURIES_NO_INDICATION|INJURIES_UNKNOWN|CRASH_HOUR|CRASH_DAY_OF_WEEK|CRASH_MONTH|    LATITUDE|    LONGITUDE|            LOCATION|\n+--------------------+----------------+--------------------+------------------+----------------------+--------------------+-----------------+------------------+--------------------+--------------------+--------+------------------+--------------------+-----------+-----------+--------------------+----------------------+------------------+-------------+-------------+--------------------+-----------------------+----------------------+---------+----------------+-----------+------------------+--------------+------------------+---------+-----------+--------------+-----------------+---------+--------------------+--------------+--------------+-----------------------+---------------------------+-----------------------------+----------------------+----------------+----------+-----------------+-----------+------------+-------------+--------------------+\n|6c1659069e9c6285a...|            null|08/18/2023 12:50:...|                15|                 OTHER|FUNCTIONING PROPERLY|            CLEAR|          DAYLIGHT|            REAR END|               OTHER|    null|STRAIGHT AND LEVEL|                 DRY| NO DEFECTS|   ON SCENE|INJURY AND / OR T...|                  null|              null|         null|  OVER $1,500|08/18/2023 12:55:...|   FOLLOWING TOO CLO...|  DISTRACTION - FRO...|      700|               W|   OHARE ST|              1654|          null|              null|     null|       null|          null|             null|        2|NONINCAPACITATING...|             1|             0|                      0|                          1|                            0|                     1|               0|        12|                6|          8|        null|         null|                null|\n|5f54a59fcb087b12a...|            null|07/29/2023 02:45:...|                30|        TRAFFIC SIGNAL|FUNCTIONING PROPERLY|            CLEAR|          DAYLIGHT|PARKED MOTOR VEHICLE|DIVIDED - W/MEDIA...|    null|STRAIGHT AND LEVEL|                 DRY| NO DEFECTS|   ON SCENE|NO INJURY / DRIVE...|                  null|              null|            Y|  OVER $1,500|07/29/2023 02:45:...|   FAILING TO REDUCE...|  OPERATING VEHICLE...|     2101|               S|ASHLAND AVE|              1235|          null|              null|     null|       null|          null|             null|        4|NO INDICATION OF ...|             0|             0|                      0|                          0|                            0|                     1|               0|        14|                7|          7|41.854120263|-87.665902343|POINT (-87.665902...|\n|61fcb8c1eb522a646...|            null|08/18/2023 05:58:...|                30|           NO CONTROLS|         NO CONTROLS|            CLEAR|          DAYLIGHT|        PEDALCYCLIST|         NOT DIVIDED|    null|STRAIGHT AND LEVEL|                 DRY| NO DEFECTS|   ON SCENE|INJURY AND / OR T...|                  null|              null|         null|$501 - $1,500|08/18/2023 06:01:...|   FAILING TO REDUCE...|   UNABLE TO DETERMINE|     3422|               N|   LONG AVE|              1633|          null|              null|     null|       null|          null|             null|        2|NONINCAPACITATING...|             1|             0|                      0|                          1|                            0|                     1|               0|        17|                6|          8|41.942975745|-87.761883497|POINT (-87.761883...|\n|004cd14d0303a9163...|            null|11/26/2019 08:38:...|                25|           NO CONTROLS|         NO CONTROLS|            CLEAR|          DAYLIGHT|          PEDESTRIAN|             ONE-WAY|    null|    CURVE ON GRADE|                 DRY| NO DEFECTS|   ON SCENE|INJURY AND / OR T...|                  null|              null|         null|  OVER $1,500|11/26/2019 08:38:...|    UNABLE TO DETERMINE|        NOT APPLICABLE|        5|               W|TERMINAL ST|              1655|             Y|                 Y|     null|       null|          null|             null|        2|               FATAL|             1|             1|                      0|                          0|                            0|                     1|               0|         8|                3|         11|        null|         null|                null|\n|a1d5f0ea908977453...|            null|08/18/2023 10:45:...|                20|           NO CONTROLS|         NO CONTROLS|            CLEAR|          DAYLIGHT|        FIXED OBJECT|               OTHER|    null|STRAIGHT AND LEVEL|                 DRY| NO DEFECTS|   ON SCENE|NO INJURY / DRIVE...|                  null|              null|         null|  OVER $1,500|08/18/2023 10:48:...|   FOLLOWING TOO CLO...|  DRIVING SKILLS/KN...|        3|               W|TERMINAL ST|              1653|          null|              null|     null|       null|          null|             null|        1|NO INDICATION OF ...|             0|             0|                      0|                          0|                            0|                     1|               0|        10|                6|          8|        null|         null|                null|\n+--------------------+----------------+--------------------+------------------+----------------------+--------------------+-----------------+------------------+--------------------+--------------------+--------+------------------+--------------------+-----------+-----------+--------------------+----------------------+------------------+-------------+-------------+--------------------+-----------------------+----------------------+---------+----------------+-----------+------------------+--------------+------------------+---------+-----------+--------------+-----------------+---------+--------------------+--------------+--------------+-----------------------+---------------------------+-----------------------------+----------------------+----------------+----------+-----------------+-----------+------------+-------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Convert the results to a Spark DataFrame\n",
    "df2 = spark.createDataFrame(results)\n",
    "print(df2.count())\n",
    "# Show the first few rows of the DataFrame\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fad5a2-1613-4cec-ae8e-1d128510cfbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create RDD of wanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d16fd3f-6dba-4801-8d28-537a47d1759f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+--------------------+--------------------+---------+--------+\n|          Crash_type|num_units|Weather_condition|          Crash_date|  Most_severe_injury|Longitude|Latitude|\n+--------------------+---------+-----------------+--------------------+--------------------+---------+--------+\n|INJURY AND / OR T...|        2|            CLEAR|08/18/2023 12:50:...|NONINCAPACITATING...|     null|    null|\n+--------------------+---------+-----------------+--------------------+--------------------+---------+--------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "wanted_columns = df2.select(\"Crash_type\",\"num_units\",\"Weather_condition\",\"Crash_date\",\"Most_severe_injury\",\"Longitude\",\"Latitude\")\n",
    "wanted_columns.show(1)\n",
    "rdd_of_features = wanted_columns.rdd.map(lambda row:[row[0],row[1],row[2],row[3],row[4],row[5],row[6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cf460a-c0c5-4d25-b736-79f06b69fc19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Remove all rows where the content of one of the fields is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8e8d5e-837d-4d7f-b335-4099e24b27db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784815\n735689\n"
     ]
    }
   ],
   "source": [
    "print(rdd_of_features.count())\n",
    "#row[0] = Crash_type, row[2] = Weather_condition,  row[4]= Most_severe_injury\n",
    "cleaned_data_rdd = rdd_of_features.filter(lambda row: row[0]!=\"UNKNOWN\"  and row[2]!=\"UNKNOWN\"  and row[4]!=\"UNKNOWN\" and row[5] != None and row[6] != None and row[0] != None and row[1] != None and row[2] != None and row[3] != None and row[4] != None)\n",
    "print(cleaned_data_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db499f2b-8f48-4a7d-8fb7-b399b739179d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Dataframe from RDD and get it ready for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6a14eb-205d-441d-884b-f6aba0d71546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_1', 'string'), ('_2', 'bigint'), ('_3', 'string'), ('_4', 'string'), ('_5', 'string'), ('_6', 'double'), ('_7', 'double')]\n[('_1', 'string'), ('_2', 'double'), ('_3', 'string'), ('_4', 'string'), ('_5', 'string'), ('_6', 'double'), ('_7', 'double')]\n+--------------------+---+-----+--------------------+--------------------+-------------+------------+--------+--------+--------+--------+\n|                  _1| _2|   _3|                  _4|                  _5|           _6|          _7|_1_index|_3_index|_4_index|_5_index|\n+--------------------+---+-----+--------------------+--------------------+-------------+------------+--------+--------+--------+--------+\n|NO INJURY / DRIVE...|4.0|CLEAR|07/29/2023 02:45:...|NO INDICATION OF ...|-87.665902343|41.854120263|     0.0|     0.0|  9668.0|     0.0|\n|INJURY AND / OR T...|2.0|CLEAR|08/18/2023 05:58:...|NONINCAPACITATING...|-87.761883497|41.942975745|     1.0|     0.0|354021.0|     1.0|\n|NO INJURY / DRIVE...|2.0|CLEAR|07/29/2023 12:50:...|NO INDICATION OF ...|-87.696642375|41.899224596|     0.0|     0.0|333318.0|     0.0|\n|NO INJURY / DRIVE...|2.0|CLEAR|09/20/2023 12:57:...|NO INDICATION OF ...|-87.585945067|41.744151639|     0.0|     0.0|389374.0|     0.0|\n|INJURY AND / OR T...|2.0|CLEAR|08/09/2023 07:55:...|NONINCAPACITATING...|-87.626521907|41.758245505|     1.0|     0.0|344809.0|     1.0|\n+--------------------+---+-----+--------------------+--------------------+-------------+------------+--------+--------+--------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cleaned_data_df = spark.createDataFrame(cleaned_data_rdd)\n",
    "\n",
    "#_1 = Crash_type, _2 = numUnits, _3 = weather, _4 = time, _5 = injury severity, _6 = longitude, _7 = latitude\n",
    "print(cleaned_data_df.dtypes)\n",
    "numeric_cols = [\"_2\", \"_6\", \"_7\"]\n",
    "for col_name in numeric_cols:    \n",
    "    cleaned_data_df = cleaned_data_df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "print(cleaned_data_df.dtypes)\n",
    "\n",
    "string_cols = [\"_1\", \"_3\", \"_4\", \"_5\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(cleaned_data_df) for column in string_cols ]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_df = pipeline.fit(cleaned_data_df).transform(cleaned_data_df)\n",
    "indexed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea5c508-7be7-4ea1-85fd-f18d16597e3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8198d30-38e7-452a-b063-3bb0fec0e10f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Labeled Points and Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd89cfb9-03ee-4929-9201-28fd888e7235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='NO INJURY / DRIVE AWAY', _2=4.0, _3='CLEAR', _4='07/29/2023 02:45:00 PM', _5='NO INDICATION OF INJURY', _6=-87.665902343, _7=41.854120263, _1_index=0.0, _3_index=0.0, _4_index=9668.0, _5_index=0.0, features=DenseVector([4.0, -87.6659, 0.0, 0.0, 9668.0, 0.0]), scaledFeatures=DenseVector([0.1765, 0.0031, 0.0, 0.0, 0.0195, 0.0]), label=41.854120263)]\n[0.1764705882352941,0.0030737128245124593,0.0,0.0,0.01951916787130432,0.0] 41.854120263\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2915852555945588>:26\u001B[0m\n",
       "\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n",
       "\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2915852555945588>:26\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n\n\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'firstPointFeatures' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StandardScaler\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "feature_column_names = [\"_2\", \"_6\", \"_1_index\", \"_3_index\", \"_4_index\", \"_5_index\"]\n",
    "\n",
    "# Assemble numeric columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_column_names, outputCol=\"features\")\n",
    "vector_df = assembler.transform(indexed_df)\n",
    "\n",
    "useMinMaxScaler=True\n",
    "if (useMinMaxScaler):\n",
    "    # Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "else:\n",
    "    # Apply StandarScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "\n",
    "normalizedSamplePoints = scaledData.withColumn(\"label\", col(\"_7\"))\n",
    "firstPoint = normalizedSamplePoints.take(1)\n",
    "print(firstPoint)\n",
    "print (firstPoint[0].scaledFeatures, firstPoint[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe5f8f8-fbd0-450c-a6d5-02929501afb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588745 146944 735689\n"
     ]
    }
   ],
   "source": [
    "def to_labeled_point(row):\n",
    "    # Convert the DenseVector to a list for features\n",
    "    features_list = row.scaledFeatures.toArray().tolist()\n",
    "    return LabeledPoint(row['label'], features_list)\n",
    "\n",
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "\n",
    "# List of column names that are not needed in df, performance optimization for conversion to rdd later\n",
    "drop_columns = [\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\",\"_7\",\"_1_index\",\"_3_index\",\"_4_index\",\"_5_index\",\"features\"]\n",
    "normalizedSamplePoints=normalizedSamplePoints.drop(*drop_columns)\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData=parsedTrainData.rdd.map(to_labeled_point)\n",
    "parsedValData=parsedValData.rdd.map(to_labeled_point)\n",
    "\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a4ba87c-f94d-4c2e-8bfe-d4daf774beeb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create baseline using the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5850b11d-0b29-4d5b-a352-370d4b0a688f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.85411854360393\n"
     ]
    }
   ],
   "source": [
    "averagelatitude = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averagelatitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd6a731-6c92-4d25-8392-2f5a88e48885",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3557268831871511\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def squaredError(label, prediction):\n",
    "    sqrError = (label-prediction)*(label-prediction)\n",
    "    return sqrError\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    sqrSum = labelsAndPreds.map(lambda s: squaredError(s[0],s[1])).sum()\n",
    "    return math.sqrt(sqrSum/labelsAndPreds.count())\n",
    "\n",
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c125c1ec-1216-4879-8ff5-9ae5567c148f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Version One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2145917a-c256-4438-ac93-eb8799c97647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81979d1d-7177-43e8-8df1-a82f8a31405b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.976201139, [0.0,0.00035120717607830393,0.0,0.0,0.09826612935789449,0.0]), LabeledPoint(41.994793386, [0.0,0.0006082283665866339,0.0,0.0,0.1666155200400559,0.0]), LabeledPoint(41.994793386, [0.0,0.0006082283665866339,0.0,0.0,0.6761610957222577,0.0]), LabeledPoint(41.977955465, [0.0,0.0010220258119903588,0.0,0.0,0.260678204268859,0.0]), LabeledPoint(41.952142587, [0.0,0.0011301226112881355,0.0,0.0,0.8405497185589573,0.0])]\n41.88699044150097\n41.88699044150097\n41.88311287092069\n41.88283808646855\n41.88311287092069\n"
     ]
    }
   ],
   "source": [
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6daa57-3abf-4bd5-a4cd-c026719687d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3557268831871511\n0.3528108420753974\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a91e76-099a-4554-99b0-a8f43a314308",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Version Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc811b0c-ff89-45e5-a873-be0e4193e986",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=6, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90052f2-3452-42b2-a114-8093a1b495f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.976201139, [0.0,0.00035120717607830393,0.0,0.0,0.09826612935789449,0.0]), LabeledPoint(41.994793386, [0.0,0.0006082283665866339,0.0,0.0,0.1666155200400559,0.0]), LabeledPoint(41.994793386, [0.0,0.0006082283665866339,0.0,0.0,0.6761610957222577,0.0]), LabeledPoint(41.977955465, [0.0,0.0010220258119903588,0.0,0.0,0.260678204268859,0.0]), LabeledPoint(41.952142587, [0.0,0.0011301226112881355,0.0,0.0,0.8405497185589573,0.0])]\n41.893968146825465\n41.893968146825465\n41.90063402583679\n41.897134107364536\n41.90186023035237\n"
     ]
    }
   ],
   "source": [
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44be81f7-4051-450a-a647-9c754c8a9594",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3557268831871511\n0.3507406269653482\n"
     ]
    }
   ],
   "source": [
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b45257-815c-4561-8caa-b9b634e539ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c9644a-41e3-4fd8-8738-1c7e00b70fa0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Labeled Points and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97780260-0bba-464f-aa5f-244815a14eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='NO INJURY / DRIVE AWAY', _2=4.0, _3='CLEAR', _4='07/29/2023 02:45:00 PM', _5='NO INDICATION OF INJURY', _6=-87.665902343, _7=41.854120263, _1_index=0.0, _3_index=0.0, _4_index=9668.0, _5_index=0.0, features=DenseVector([4.0, 41.8541, 0.0, 0.0, 9668.0, 0.0]), scaledFeatures=DenseVector([0.1765, 0.996, 0.0, 0.0, 0.0195, 0.0]), label=-87.665902343)]\n[0.1764705882352941,0.995986472133498,0.0,0.0,0.01951916787130432,0.0] -87.665902343\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2915852555945612>:23\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n",
       "\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2915852555945612>:23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n\n\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'firstPointFeatures' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_column_names = [\"_2\", \"_7\", \"_1_index\", \"_3_index\", \"_4_index\", \"_5_index\"]\n",
    "\n",
    "# Assemble numeric columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_column_names, outputCol=\"features\")\n",
    "vector_df = assembler.transform(indexed_df)\n",
    "\n",
    "useMinMaxScaler=True\n",
    "if (useMinMaxScaler):\n",
    "    # Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "else:\n",
    "    # Apply StandarScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "\n",
    "normalizedSamplePoints = scaledData.withColumn(\"label\", col(\"_6\"))\n",
    "firstPoint = normalizedSamplePoints.take(1)\n",
    "print(firstPoint)\n",
    "print (firstPoint[0].scaledFeatures, firstPoint[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e4af31-483a-4c14-8742-c8edd1eaa7a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588745 146944 735689\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "\n",
    "# List of column names that are not needed in df, performance optimization for conversion to rdd later\n",
    "drop_columns = [\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\",\"_7\",\"_1_index\",\"_3_index\",\"_4_index\",\"_5_index\",\"features\"]\n",
    "normalizedSamplePoints=normalizedSamplePoints.drop(*drop_columns)\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData=parsedTrainData.rdd.map(to_labeled_point)\n",
    "parsedValData=parsedValData.rdd.map(to_labeled_point)\n",
    "\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4bb7a25-327a-4111-8053-d731539a353e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11bb2efc-e629-4273-83f1-6ceee8980ffb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-87.67353957861799\n"
     ]
    }
   ],
   "source": [
    "averagelongitude = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averagelongitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5d96cd-ce97-455e-9a70-64def796ce69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.5279791671811\n"
     ]
    }
   ],
   "source": [
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff528d1f-e05e-4aa5-9f9f-b1a39cd8f775",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0795c820-0985-43b2-8849-e98cd2d11ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.0318730971435955,0.0]), LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.08369337866539608,0.0]), LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.21620890435850018,0.0]), LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.4414788374102579,0.0]), LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.7866620365509946,0.0])]\n-87.55268607925034\n-87.55268607925034\n-87.53376281848195\n-87.48414517085668\n-87.48414517085668\n129.5279791671811\n0.7933934493548133\n"
     ]
    }
   ],
   "source": [
    "longitudeOne = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = longitudeOne.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = longitudeOne.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9ee929-dab2-4b33-bac1-b904e23d6cb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff255a0-a774-4602-a4b2-19fd8e352e66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.0318730971435955,0.0]), LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.08369337866539608,0.0]), LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.21620890435850018,0.0]), LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.4414788374102579,0.0]), LabeledPoint(0.0, [0.058823529411764705,0.0,0.0,0.0,0.7866620365509946,0.0])]\n-87.49483083634316\n-87.54332104844782\n-87.54998009262786\n-87.5416644386359\n-87.55762057431751\n129.5279791671811\n0.7933588810407468\n"
     ]
    }
   ],
   "source": [
    "longitudeTwo = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=6, maxBins=32)\n",
    "\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = longitudeTwo.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = longitudeTwo.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d09a916-d612-48d2-b92b-27cde8c009da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Type of Crash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1334c40f-1f77-4234-a66d-a4f197dc541b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Labeled Points and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14ac63e7-4657-4992-ae52-d0d870e7925d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='NO INJURY / DRIVE AWAY', _2=4.0, _3='CLEAR', _4='07/29/2023 02:45:00 PM', _5='NO INDICATION OF INJURY', _6=-87.665902343, _7=41.854120263, _1_index=0.0, _3_index=0.0, _4_index=9668.0, _5_index=0.0, features=DenseVector([4.0, 41.8541, -87.6659, 0.0, 9668.0, 0.0]), scaledFeatures=DenseVector([0.1765, 0.996, 0.0031, 0.0, 0.0195, 0.0]), label=0.0)]\n[0.1764705882352941,0.995986472133498,0.0030737128245124593,0.0,0.01951916787130432,0.0] 0.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2915852555945623>:23\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n",
       "\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2915852555945623>:23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n\n\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'firstPointFeatures' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_column_names = [\"_2\", \"_7\", \"_6\", \"_3_index\", \"_4_index\", \"_5_index\"]\n",
    "\n",
    "# Assemble numeric columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_column_names, outputCol=\"features\")\n",
    "vector_df = assembler.transform(indexed_df)\n",
    "\n",
    "useMinMaxScaler=True\n",
    "if (useMinMaxScaler):\n",
    "    # Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "else:\n",
    "    # Apply StandarScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "\n",
    "normalizedSamplePoints = scaledData.withColumn(\"label\", col(\"_1_index\"))\n",
    "firstPoint = normalizedSamplePoints.take(1)\n",
    "print(firstPoint)\n",
    "print (firstPoint[0].scaledFeatures, firstPoint[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "215030b8-535e-4c84-9e60-411d9c06bf0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588745 146944 735689\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "\n",
    "# List of column names that are not needed in df, performance optimization for conversion to rdd later\n",
    "drop_columns = [\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\",\"_7\",\"_1_index\",\"_3_index\",\"_4_index\",\"_5_index\",\"features\"]\n",
    "normalizedSamplePoints=normalizedSamplePoints.drop(*drop_columns)\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData=parsedTrainData.rdd.map(to_labeled_point)\n",
    "parsedValData=parsedValData.rdd.map(to_labeled_point)\n",
    "\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "752fe70a-a826-4132-b14e-3e97560a7243",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0435fdf9-089d-478b-b583-99d8eb1feb82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2749730358644238\n"
     ]
    }
   ],
   "source": [
    "averagetype = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averagetype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e995d1-1964-4547-ad53-b28f27c1f879",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.58225778891926\n"
     ]
    }
   ],
   "source": [
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc914eec-24c1-45bf-8493-c91e4aa89660",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb877f98-cc1d-4e9b-8720-518a0571d25d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(1.0, [0.0,0.991134582570875,0.0044363947076390825,0.0,0.23895030970628378,0.0]), LabeledPoint(0.0, [0.0,0.9912085289164112,0.0036286714981203774,0.0,0.07289807554087559,0.0]), LabeledPoint(1.0, [0.0,0.9912185808692187,0.004632042556646592,0.2,0.31060673358798974,0.0]), LabeledPoint(0.0, [0.0,0.9912581380571414,0.00439508509576857,0.1,0.8348825377340968,0.0]), LabeledPoint(0.0, [0.0,0.9912825164062985,0.0036586329271032776,0.0,0.26806148901289706,0.0])]\n0.30061708627288575\n0.29565314114612884\n0.5635406993969293\n0.5734715988130848\n0.449712152625503\n41.58225778891926\n0.3207803221909808\n"
     ]
    }
   ],
   "source": [
    "typeOne = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = typeOne.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = typeOne.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce07de60-980a-4320-9499-3351517c2d52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86badd95-b51c-4b9b-85f0-c732bd74e1a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(1.0, [0.0,0.991134582570875,0.0044363947076390825,0.0,0.23895030970628378,0.0]), LabeledPoint(0.0, [0.0,0.9912085289164112,0.0036286714981203774,0.0,0.07289807554087559,0.0]), LabeledPoint(1.0, [0.0,0.9912185808692187,0.004632042556646592,0.2,0.31060673358798974,0.0]), LabeledPoint(0.0, [0.0,0.9912581380571414,0.00439508509576857,0.1,0.8348825377340968,0.0]), LabeledPoint(0.0, [0.0,0.9912825164062985,0.0036586329271032776,0.0,0.26806148901289706,0.0])]\n0.2980893301573213\n0.29653073331398383\n0.4513659525869353\n0.46016532504909846\n0.47242738036928256\n41.58225778891926\n0.3281357779519216\n"
     ]
    }
   ],
   "source": [
    "typeTwo = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = typeTwo.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = typeTwo.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca17366-0289-4aec-b895-f058986b32c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a67992-78b3-4cc9-b85f-6ef7c9cdda50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Labeled Points and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bf7441b-6b26-4943-9d31-b5f716d48bd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='NO INJURY / DRIVE AWAY', _2=4.0, _3='CLEAR', _4='07/29/2023 02:45:00 PM', _5='NO INDICATION OF INJURY', _6=-87.665902343, _7=41.854120263, _1_index=0.0, _3_index=0.0, _4_index=9668.0, _5_index=0.0, features=DenseVector([4.0, 41.8541, -87.6659, 0.0, 9668.0, 0.0]), scaledFeatures=DenseVector([0.1765, 0.996, 0.0031, 0.0, 0.0195, 0.0]), label=0.0)]\n[0.1764705882352941,0.995986472133498,0.0030737128245124593,0.0,0.01951916787130432,0.0] 0.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2915852555945630>:23\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n",
       "\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2915852555945630>:23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n\n\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'firstPointFeatures' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_column_names = [\"_2\", \"_7\", \"_6\", \"_1_index\", \"_4_index\", \"_5_index\"]\n",
    "\n",
    "# Assemble numeric columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_column_names, outputCol=\"features\")\n",
    "vector_df = assembler.transform(indexed_df)\n",
    "\n",
    "useMinMaxScaler=True\n",
    "if (useMinMaxScaler):\n",
    "    # Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "else:\n",
    "    # Apply StandarScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "\n",
    "normalizedSamplePoints = scaledData.withColumn(\"label\", col(\"_3_index\"))\n",
    "firstPoint = normalizedSamplePoints.take(1)\n",
    "print(firstPoint)\n",
    "print (firstPoint[0].scaledFeatures, firstPoint[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e44b9f-35d1-48e2-b3c7-f341a5f7805f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588745 146944 735689\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "\n",
    "# List of column names that are not needed in df, performance optimization for conversion to rdd later\n",
    "drop_columns = [\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\",\"_7\",\"_1_index\",\"_3_index\",\"_4_index\",\"_5_index\",\"features\"]\n",
    "normalizedSamplePoints=normalizedSamplePoints.drop(*drop_columns)\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData=parsedTrainData.rdd.map(to_labeled_point)\n",
    "parsedValData=parsedValData.rdd.map(to_labeled_point)\n",
    "\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4568f334-1e60-4f53-9d80-550057259b1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0a8eb2-d103-42ff-b7ed-a5b1bae1046d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3019133920457928\n"
     ]
    }
   ],
   "source": [
    "averageweather = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averageweather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667ad8f9-aada-4996-9fa6-fc07f6a40e93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.56084745132242\n"
     ]
    }
   ],
   "source": [
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60da4fef-abc0-443c-8324-2377d0667e4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3084ae26-0c71-4901-8b6a-e2cecaeaacf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.0,0.991134582570875,0.0044363947076390825,1.0,0.23895030970628378,0.0]), LabeledPoint(0.0, [0.0,0.9912085289164112,0.0036286714981203774,0.0,0.07289807554087559,0.0]), LabeledPoint(2.0, [0.0,0.9912185808692187,0.004632042556646592,1.0,0.31060673358798974,0.0]), LabeledPoint(1.0, [0.0,0.9912581380571414,0.00439508509576857,0.0,0.8348825377340968,0.0]), LabeledPoint(0.0, [0.0,0.9912825164062985,0.0036586329271032776,0.0,0.26806148901289706,0.0])]\n0.43602195203080163\n0.39302928882855515\n0.7120867050423874\n0.30775897044345824\n0.5439281077097404\n41.56084745132242\n0.8188218608003667\n"
     ]
    }
   ],
   "source": [
    "weatherOne = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = weatherOne.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = weatherOne.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "550cfdf9-f444-4ee4-8a13-331fcba811cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae812c59-35df-4be5-afc6-6b542b51b5b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.0,0.991134582570875,0.0044363947076390825,1.0,0.23895030970628378,0.0]), LabeledPoint(0.0, [0.0,0.9912085289164112,0.0036286714981203774,0.0,0.07289807554087559,0.0]), LabeledPoint(2.0, [0.0,0.9912185808692187,0.004632042556646592,1.0,0.31060673358798974,0.0]), LabeledPoint(1.0, [0.0,0.9912581380571414,0.00439508509576857,0.0,0.8348825377340968,0.0]), LabeledPoint(0.0, [0.0,0.9912825164062985,0.0036586329271032776,0.0,0.26806148901289706,0.0])]\n0.40712059261811023\n0.3484273402624314\n0.6326541867915426\n0.36246482545027275\n0.47074187778286314\n41.56084745132242\n0.8185131741214071\n"
     ]
    }
   ],
   "source": [
    "weatherTwo = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = weatherTwo.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = weatherTwo.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2802bcb-b761-495f-98ec-85abdc71000c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Injury Severity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a170f50-ad16-461e-af09-c17bbf1a0189",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Laeled Points and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5f9cb4-ef07-476a-8b6d-2944f754757b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='NO INJURY / DRIVE AWAY', _2=4.0, _3='CLEAR', _4='07/29/2023 02:45:00 PM', _5='NO INDICATION OF INJURY', _6=-87.665902343, _7=41.854120263, _1_index=0.0, _3_index=0.0, _4_index=9668.0, _5_index=0.0, features=DenseVector([4.0, 41.8541, -87.6659, 0.0, 0.0, 0.0]), scaledFeatures=DenseVector([0.1765, 0.996, 0.0031, 0.0, 0.0, 0.0]), label=9668.0)]\n[0.1764705882352941,0.995986472133498,0.0030737128245124593,0.0,0.0,0.0] 9668.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2915852555945637>:23\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n",
       "\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2915852555945637>:23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(firstPoint)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m (firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mscaledFeatures, firstPoint[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlabel)\n\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(firstPointFeatures))\n\n\u001B[0;31mNameError\u001B[0m: name 'firstPointFeatures' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'firstPointFeatures' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_column_names = [\"_2\", \"_7\", \"_6\", \"_1_index\", \"_3_index\", \"_5_index\"]\n",
    "\n",
    "# Assemble numeric columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_column_names, outputCol=\"features\")\n",
    "vector_df = assembler.transform(indexed_df)\n",
    "\n",
    "useMinMaxScaler=True\n",
    "if (useMinMaxScaler):\n",
    "    # Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "else:\n",
    "    # Apply StandarScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "\n",
    "normalizedSamplePoints = scaledData.withColumn(\"label\", col(\"_4_index\"))\n",
    "firstPoint = normalizedSamplePoints.take(1)\n",
    "print(firstPoint)\n",
    "print (firstPoint[0].scaledFeatures, firstPoint[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2d0c29-929a-4301-a4a3-0c368e3b66ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "\n",
    "# List of column names that are not needed in df, performance optimization for conversion to rdd later\n",
    "drop_columns = [\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\",\"_7\",\"_1_index\",\"_3_index\",\"_4_index\",\"_5_index\",\"features\"]\n",
    "normalizedSamplePoints=normalizedSamplePoints.drop(*drop_columns)\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData=parsedTrainData.rdd.map(to_labeled_point)\n",
    "parsedValData=parsedValData.rdd.map(to_labeled_point)\n",
    "\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f781ac04-f531-481a-99d2-6b0320b742df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb671acd-e980-467b-98ed-650461a014e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "averageinjury = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averageinjury)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a253c3e4-a7f7-4e8d-b7e8-adb59b9867bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88d2fb55-e45b-4baa-b00b-59495e43a528",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52c16fd-490a-4de2-9264-952bb62c72cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "injuryOne = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = injuryOne.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = injuryOne.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43332f5b-ff90-439f-8593-f396bf886c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff627f90-4677-436e-8964-9c73ab807998",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "injuryTwo = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = injuryTwo.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = injuryTwo.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f7dd4f-5375-40b5-9f1e-5049d1225d37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Number of Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee7a5f08-ad5a-4ef5-9d8e-a98cd140218c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Labeled Points and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036ce71c-a25e-45b3-b755-5cf422c565dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_column_names = [\"_7\", \"_6\", \"_1_index\", \"_3_index\", \"_5_index\",\"_4_index\"]\n",
    "\n",
    "# Assemble numeric columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_column_names, outputCol=\"features\")\n",
    "vector_df = assembler.transform(indexed_df)\n",
    "\n",
    "useMinMaxScaler=True\n",
    "if (useMinMaxScaler):\n",
    "    # Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "else:\n",
    "    # Apply StandarScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "\n",
    "normalizedSamplePoints = scaledData.withColumn(\"label\", col(\"_2\"))\n",
    "firstPoint = normalizedSamplePoints.take(1)\n",
    "print(firstPoint)\n",
    "print (firstPoint[0].scaledFeatures, firstPoint[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7ccfc3-544d-493d-832b-6ebc1179ab68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "\n",
    "# List of column names that are not needed in df, performance optimization for conversion to rdd later\n",
    "drop_columns = [\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\",\"_7\",\"_1_index\",\"_3_index\",\"_4_index\",\"_5_index\",\"features\"]\n",
    "normalizedSamplePoints=normalizedSamplePoints.drop(*drop_columns)\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData=parsedTrainData.rdd.map(to_labeled_point)\n",
    "parsedValData=parsedValData.rdd.map(to_labeled_point)\n",
    "\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db2e57d-0957-405f-97e5-53e43e99162f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b81c2d-77e7-4572-a57d-546f27afeea7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "averagenumber = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averagenumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388b2a5e-0bc0-4f42-8c5d-8f2425d6ba28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagenumber))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagenumber))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa151c35-3ddb-4770-a66d-dc99285e5ea4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2779fc-a189-4792-920b-02807269d802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numberOne = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = numberOne.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = numberOne.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5970aac5-1d8f-4316-812e-4c1e8f957d2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeae374f-9ac7-4bc3-be40-110bcfe44c15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numberTwo = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = numberTwo.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = numberTwo.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c322ed-fa5e-4c86-b6cc-38de38c7fb9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Time of Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7bcfab4-141e-41a0-870d-9333769fa297",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Labeled Points and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db70ad33-be43-471b-9971-272316b4dcdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_column_names = [\"_7\", \"_6\", \"_1_index\", \"_3_index\", \"_2\",\"_4_index\"]\n",
    "\n",
    "# Assemble numeric columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_column_names, outputCol=\"features\")\n",
    "vector_df = assembler.transform(indexed_df)\n",
    "\n",
    "useMinMaxScaler=True\n",
    "if (useMinMaxScaler):\n",
    "    # Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "else:\n",
    "    # Apply StandarScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scalerModel = scaler.fit(vector_df)\n",
    "    scaledData = scalerModel.transform(vector_df)\n",
    "\n",
    "normalizedSamplePoints = scaledData.withColumn(\"label\", col(\"_5_index\"))\n",
    "firstPoint = normalizedSamplePoints.take(1)\n",
    "print(firstPoint)\n",
    "print (firstPoint[0].scaledFeatures, firstPoint[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d014aaf9-117f-4142-84e9-45e033384ec1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "\n",
    "# List of column names that are not needed in df, performance optimization for conversion to rdd later\n",
    "drop_columns = [\"_1\", \"_2\", \"_3\", \"_4\", \"_5\", \"_6\",\"_7\",\"_1_index\",\"_3_index\",\"_4_index\",\"_5_index\",\"features\"]\n",
    "normalizedSamplePoints=normalizedSamplePoints.drop(*drop_columns)\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData=parsedTrainData.rdd.map(to_labeled_point)\n",
    "parsedValData=parsedValData.rdd.map(to_labeled_point)\n",
    "\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a1312d0-95d4-4207-b67a-ed8828d60b2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1d5316-384c-4053-82e4-1cf6115defd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "averagetime = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averagetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2dfc642-3f6d-49ed-9ff6-6c839dfd31e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagetime))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagetime))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c3b290e-fef3-46a2-af35-75783b25d1c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07563551-c30e-452c-a733-308139f3d468",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeOne = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = timeOne.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = timeOne.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca5c4632-6d59-4337-8efb-77a793af6efc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Random Forest Version Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79382cb-400d-4021-8e9c-59527c0b0c9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timeTwo = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = timeTwo.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = timeTwo.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Traffic Crash Analysis All Models",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
