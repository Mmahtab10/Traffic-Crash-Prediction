{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2e8825-b1d2-4c6d-81ee-75c05f88d00c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Traffic Crash Analysis\n",
    "\n",
    "### Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3229176b-3990-44da-9583-71919c5cd60e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Make the GET request\n",
    "resp = requests.get('https://data.cityofchicago.org/resource/85ca-t3if.json?$query=SELECT%20crash_record_id%2C%20crash_date_est_i%2C%20crash_date%2C%20posted_speed_limit%2C%20traffic_control_device%2C%20device_condition%2C%20weather_condition%2C%20lighting_condition%2C%20first_crash_type%2C%20trafficway_type%2C%20lane_cnt%2C%20alignment%2C%20roadway_surface_cond%2C%20road_defect%2C%20report_type%2C%20crash_type%2C%20intersection_related_i%2C%20private_property_i%2C%20hit_and_run_i%2C%20damage%2C%20date_police_notified%2C%20prim_contributory_cause%2C%20sec_contributory_cause%2C%20street_no%2C%20street_direction%2C%20street_name%2C%20beat_of_occurrence%2C%20photos_taken_i%2C%20statements_taken_i%2C%20dooring_i%2C%20work_zone_i%2C%20work_zone_type%2C%20workers_present_i%2C%20num_units%2C%20most_severe_injury%2C%20injuries_total%2C%20injuries_fatal%2C%20injuries_incapacitating%2C%20injuries_non_incapacitating%2C%20injuries_reported_not_evident%2C%20injuries_no_indication%2C%20injuries_unknown%2C%20crash_hour%2C%20crash_day_of_week%2C%20crash_month%2C%20latitude%2C%20longitude%2C%20location%20ORDER%20BY%20crash_date%20DESC%2C%20crash_record_id%20ASC')\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SENG550\").getOrCreate()\n",
    "#df2 = spark.read.csv(\"/FileStore/tables/Traffic_Crashes___Crashes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a Spark DataFrame from the response text\n",
    "df2 = spark.read.json(spark.sparkContext.parallelize([resp.text]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb8512f-928f-45de-8a07-38be8594f2e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+----------------+-----------------+----------+-----------+--------------------+--------------------+------------+--------------------+--------------------+---------+--------------------+-------------+--------------+-----------------------+----------------------+---------------------------+-----------------------------+--------------+----------------+----------------------+------------+--------------------+--------------------+-------------+--------------------+---------+--------------+------------------+-----------------------+------------------+--------------------+-----------+--------------------+----------------------+------------------+----------------+-----------+---------+----------------------+--------------------+-----------------+-----------+--------------+-----------------+\n|         alignment|beat_of_occurrence|          crash_date|crash_date_est_i|crash_day_of_week|crash_hour|crash_month|     crash_record_id|          crash_type|      damage|date_police_notified|    device_condition|dooring_i|    first_crash_type|hit_and_run_i|injuries_fatal|injuries_incapacitating|injuries_no_indication|injuries_non_incapacitating|injuries_reported_not_evident|injuries_total|injuries_unknown|intersection_related_i|    latitude|  lighting_condition|            location|    longitude|  most_severe_injury|num_units|photos_taken_i|posted_speed_limit|prim_contributory_cause|private_property_i|         report_type|road_defect|roadway_surface_cond|sec_contributory_cause|statements_taken_i|street_direction|street_name|street_no|traffic_control_device|     trafficway_type|weather_condition|work_zone_i|work_zone_type|workers_present_i|\n+------------------+------------------+--------------------+----------------+-----------------+----------+-----------+--------------------+--------------------+------------+--------------------+--------------------+---------+--------------------+-------------+--------------+-----------------------+----------------------+---------------------------+-----------------------------+--------------+----------------+----------------------+------------+--------------------+--------------------+-------------+--------------------+---------+--------------+------------------+-----------------------+------------------+--------------------+-----------+--------------------+----------------------+------------------+----------------+-----------+---------+----------------------+--------------------+-----------------+-----------+--------------+-----------------+\n|STRAIGHT AND LEVEL|               822|2023-12-12T02:04:...|            null|                3|         2|         12|4776824730cc7d96c...|NO INJURY / DRIVE...|$500 OR LESS|2023-12-12T02:11:...|         NO CONTROLS|     null|PARKED MOTOR VEHICLE|         null|             0|                      0|                     1|                          0|                            0|             0|               0|                  null|41.795222579|DARKNESS, LIGHTED...|{[-87.70412696178...|-87.704126962|NO INDICATION OF ...|        2|          null|                30|    UNABLE TO DETERMINE|              null|            ON SCENE| NO DEFECTS|                 DRY|   UNABLE TO DETERMINE|              null|               W|    54TH ST|     3211|           NO CONTROLS|             ONE-WAY|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|              1221|2023-12-12T01:30:...|            null|                3|         1|         12|f12c305f2c3cad0c9...|NO INJURY / DRIVE...| OVER $1,500|2023-12-12T01:39:...|         NO CONTROLS|     null|SIDESWIPE SAME DI...|         null|             0|                      0|                     3|                          0|                            0|             0|               0|                  null|41.889498193|DARKNESS, LIGHTED...|{[-87.67671834487...|-87.676718345|NO INDICATION OF ...|        2|          null|                30|    UNABLE TO DETERMINE|              null|NOT ON SCENE (DES...| NO DEFECTS|                 DRY|   UNABLE TO DETERMINE|              null|               N|  DAMEN AVE|      425|           NO CONTROLS|         NOT DIVIDED|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|               322|2023-12-11T21:45:...|            null|                2|        21|         12|785d6043550e32be5...|INJURY AND / OR T...| OVER $1,500|2023-12-11T22:45:...|         NO CONTROLS|     null|               ANGLE|         null|             0|                      0|                     1|                          0|                            1|             1|               0|                  null|41.769434236|DARKNESS, LIGHTED...|{[-87.61382711513...|-87.613827115|REPORTED, NOT EVI...|        2|          null|                30|   DISTRACTION - FRO...|              null|NOT ON SCENE (DES...| NO DEFECTS|                 DRY|        NOT APPLICABLE|              null|               S|ANTHONY AVE|     6898|     STOP SIGN/FLASHER|DIVIDED - W/MEDIA...|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|               935|2023-12-11T20:48:...|            null|                2|        20|         12|d01c4d7ecdfea74ea...|NO INJURY / DRIVE...| OVER $1,500|2023-12-11T20:50:...|FUNCTIONING PROPERLY|     null|               ANGLE|         null|             0|                      0|                     2|                          0|                            0|             0|               0|                     Y|41.809095821|DARKNESS, LIGHTED...|{[-87.63273119702...|-87.632731197|NO INDICATION OF ...|        2|          null|                30|   DISREGARDING TRAF...|              null|            ON SCENE| NO DEFECTS|                 DRY|        NOT APPLICABLE|              null|               S|   WELLS ST|     4700|        TRAFFIC SIGNAL|            FOUR WAY|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|              1421|2023-12-11T20:31:...|            null|                2|        20|         12|48961b8b0f5316859...|INJURY AND / OR T...| OVER $1,500|2023-12-11T20:31:...|FUNCTIONING PROPERLY|     null|             TURNING|            Y|             0|                      0|                     1|                          1|                            0|             1|               0|                     Y|41.915626928|            DAYLIGHT|{[-87.70677007489...|-87.706770075|NONINCAPACITATING...|        2|          null|                30|   IMPROPER OVERTAKI...|              null|            ON SCENE|    UNKNOWN|                 DRY|   UNABLE TO DETERMINE|              null|               N| KEDZIE AVE|     1901|     STOP SIGN/FLASHER|DIVIDED - W/MEDIA...|            CLEAR|       null|          null|             null|\n+------------------+------------------+--------------------+----------------+-----------------+----------+-----------+--------------------+--------------------+------------+--------------------+--------------------+---------+--------------------+-------------+--------------+-----------------------+----------------------+---------------------------+-----------------------------+--------------+----------------+----------------------+------------+--------------------+--------------------+-------------+--------------------+---------+--------------+------------------+-----------------------+------------------+--------------------+-----------+--------------------+----------------------+------------------+----------------+-----------+---------+----------------------+--------------------+-----------------+-----------+--------------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fad5a2-1613-4cec-ae8e-1d128510cfbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create RDD of wanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d16fd3f-6dba-4801-8d28-537a47d1759f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+--------------------+--------------------+-------------+------------+\n|          Crash_type|num_units|Weather_condition|          Crash_date|  Most_severe_injury|    Longitude|    Latitude|\n+--------------------+---------+-----------------+--------------------+--------------------+-------------+------------+\n|NO INJURY / DRIVE...|        2|            CLEAR|2023-12-12T02:04:...|NO INDICATION OF ...|-87.704126962|41.795222579|\n+--------------------+---------+-----------------+--------------------+--------------------+-------------+------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "wanted_columns = df2.select(\"Crash_type\",\"num_units\",\"Weather_condition\",\"Crash_date\",\"Most_severe_injury\",\"Longitude\",\"Latitude\")\n",
    "wanted_columns.show(1)\n",
    "rdd_of_features = wanted_columns.rdd.map(lambda row:[row[0],row[1],row[2],row[3],row[4],row[5],row[6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cf460a-c0c5-4d25-b736-79f06b69fc19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Remove all rows where the content of one of the fields is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8e8d5e-837d-4d7f-b335-4099e24b27db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n912\n"
     ]
    }
   ],
   "source": [
    "print(rdd_of_features.count())\n",
    "#row[0] = Crash_type, row[2] = Weather_condition,  row[4]= Most_severe_injury\n",
    "cleaned_data_rdd = rdd_of_features.filter(lambda row: row[0]!=\"UNKNOWN\"  and row[2]!=\"UNKNOWN\"  and row[4]!=\"UNKNOWN\" and row[5] != None and row[6] != None and row[0] != None and row[1] != None and row[2] != None and row[3] != None and row[4] != None)\n",
    "print(cleaned_data_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db499f2b-8f48-4a7d-8fb7-b399b739179d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Dataframe from RDD and get it ready for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6a14eb-205d-441d-884b-f6aba0d71546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_1', 'string'), ('_2', 'string'), ('_3', 'string'), ('_4', 'string'), ('_5', 'string'), ('_6', 'string'), ('_7', 'string')]\n[('_1', 'string'), ('_2', 'double'), ('_3', 'string'), ('_4', 'string'), ('_5', 'string'), ('_6', 'double'), ('_7', 'double')]\n+--------------------+---+-----+--------------------+--------------------+-------------+------------+--------+--------+--------+--------+\n|                  _1| _2|   _3|                  _4|                  _5|           _6|          _7|_1_index|_3_index|_4_index|_5_index|\n+--------------------+---+-----+--------------------+--------------------+-------------+------------+--------+--------+--------+--------+\n|NO INJURY / DRIVE...|2.0|CLEAR|2023-12-12T02:04:...|NO INDICATION OF ...|-87.704126962|41.795222579|     0.0|     0.0|   699.0|     0.0|\n|NO INJURY / DRIVE...|2.0|CLEAR|2023-12-12T01:30:...|NO INDICATION OF ...|-87.676718345|41.889498193|     0.0|     0.0|   698.0|     0.0|\n|INJURY AND / OR T...|2.0|CLEAR|2023-12-11T21:45:...|REPORTED, NOT EVI...|-87.613827115|41.769434236|     1.0|     0.0|   697.0|     2.0|\n|NO INJURY / DRIVE...|2.0|CLEAR|2023-12-11T20:48:...|NO INDICATION OF ...|-87.632731197|41.809095821|     0.0|     0.0|   696.0|     0.0|\n|INJURY AND / OR T...|2.0|CLEAR|2023-12-11T20:31:...|NONINCAPACITATING...|-87.706770075|41.915626928|     1.0|     0.0|   695.0|     1.0|\n+--------------------+---+-----+--------------------+--------------------+-------------+------------+--------+--------+--------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cleaned_data_df = spark.createDataFrame(cleaned_data_rdd)\n",
    "\n",
    "#_1 = Crash_type, _2 = numUnits, _3 = weather, _4 = time, _5 = injury severity, _6 = longitude, _7 = latitude\n",
    "print(cleaned_data_df.dtypes)\n",
    "numeric_cols = [\"_2\", \"_6\", \"_7\"]\n",
    "for col_name in numeric_cols:    \n",
    "    cleaned_data_df = cleaned_data_df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "print(cleaned_data_df.dtypes)\n",
    "\n",
    "string_cols = [\"_1\", \"_3\", \"_4\", \"_5\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(cleaned_data_df) for column in string_cols ]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_df = pipeline.fit(cleaned_data_df).transform(cleaned_data_df)\n",
    "indexed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8198d30-38e7-452a-b063-3bb0fec0e10f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Labeled Points\n",
    "\n",
    "Partially taken from lab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c911718f-e575-4df5-9983-4a77ea6dd0f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "indexed_rdd = indexed_df.rdd\n",
    "print(indexed_rdd.take(1)[0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e63b46b-d1c4-4968-aefb-c17b5aa0262d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['41.795222579,2.0,-87.704126962,0.0,0.0,699.0,0.0']\n[2.0,-87.704126962,0.0,0.0,699.0,0.0] 41.795222579\n6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def createRDD(values):\n",
    "    return str(values[6]) +',' +str(values[1]) +',' +str(values[5]) +',' +str(values[7]) +',' +str(values[8]) +',' + str(values[9]) +',' + str(values[10])\n",
    "\n",
    "def parsePoint(line):\n",
    "    label_features = line.split(',')\n",
    "    ret_val = LabeledPoint(label_features[0],label_features[1:])\n",
    "    return ret_val\n",
    "\n",
    "indexed_rdd = indexed_df.rdd\n",
    "nice_rdd = indexed_rdd.map(createRDD)\n",
    "print(nice_rdd.take(1))\n",
    "parsedPoints = nice_rdd.map(parsePoint)\n",
    "firstPoint = parsedPoints.take(1)\n",
    "firstPointFeatures =firstPoint[0].features \n",
    "firstPointLabel = firstPoint[0].label\n",
    "print (firstPointFeatures, firstPointLabel)\n",
    "d = len(firstPointFeatures)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187bc96a-b237-4ef8-9ea2-4442f3f3c240",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Normalize features\n",
    "\n",
    "Taken from lab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5b83d8-b35e-4230-a88b-ad5a8e13f731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.795222579, [-0.11729077624163668,-0.40893143042628277,-0.7001400420140046,-0.2740935175684388,1.9165425110497238,-0.4050394488109772]), LabeledPoint(41.889498193, [-0.11729077624163668,0.06470797627852717,-0.7001400420140046,-0.2740935175684388,1.9119730994124526,-0.4050394488109772]), LabeledPoint(41.769434236, [-0.11729077624163668,1.1515108917311718,1.4282856857085708,-0.2740935175684388,1.9074036877751814,2.712226182544385]), LabeledPoint(41.809095821, [-0.11729077624163668,0.8248355916686707,-0.7001400420140046,-0.2740935175684388,1.9028342761379102,-0.4050394488109772]), LabeledPoint(41.915626928, [-0.11729077624163668,-0.45460620756549647,1.4282856857085708,-0.2740935175684388,1.898264864500639,1.153593366866704])]\n"
     ]
    }
   ],
   "source": [
    "def normalizeFeatures(lp):\n",
    "    \"\"\"Normalizes features in the LabeledPoint object lp.\n",
    "\n",
    "    Args:\n",
    "        lp - LabeledPoint object \n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The object contains the label and the normalized features\n",
    "    \"\"\"\n",
    "    normalizedFeatures = list()\n",
    "    for i in range(0,len(lp.features)):\n",
    "        feature = (lp.features[i]-broadcastMean.value[i])/broadcastStdev.value[i]\n",
    "        normalizedFeatures.insert(i,feature)\n",
    "    return LabeledPoint(lp.label, normalizedFeatures)\n",
    "    #normalizedAccidents = (lp.features[0]-broadcastMeanAccidents.value)/broadcastStdevAccidents.value\n",
    "    #normalizedSnoOnGround = (lp.features[1]-broadcastMeanSnoOnGround.value)/broadcastStdevSnoOnGround.value\n",
    "    #return LabeledPoint(lp.label,[normalizedAccidents,normalizedSnoOnGround])\n",
    "\n",
    "def getNormalizedRDD(nonNormalizedRDD): \n",
    "    \"\"\"Normalizes the features of the LabeldPoints contained in nonNormalizedRDD.\n",
    "\n",
    "    Args:\n",
    "        nonNormalizedRDD - RDD containing non-normalized features \n",
    "\n",
    "    Returns:\n",
    "        returnRDD: RDD containing normalized features\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    meanList = list()\n",
    "    stdevList = list()\n",
    "    numFeatures = len(nonNormalizedRDD.take(1)[0].features)\n",
    "    for i in range(0,numFeatures):\n",
    "        featureRDD = nonNormalizedRDD.map(lambda lp: lp.features[i])\n",
    "        featureMean = featureRDD.mean()\n",
    "        featureStdev = featureRDD.stdev()\n",
    "        meanList.insert(i,featureMean)\n",
    "        stdevList.insert(i,featureStdev)\n",
    "    global broadcastMean \n",
    "    broadcastMean = sc.broadcast(meanList)\n",
    "    global broadcastStdev \n",
    "    broadcastStdev = sc.broadcast(stdevList)\n",
    "    returnRDD = nonNormalizedRDD.map(normalizeFeatures)\n",
    "    return returnRDD\n",
    "\n",
    "normalizedSamplePoints = getNormalizedRDD(parsedPoints)\n",
    "print(normalizedSamplePoints.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe5f8f8-fbd0-450c-a6d5-02929501afb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708 204 912\n912\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)\n",
    "print(normalizedSamplePoints.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a4ba87c-f94d-4c2e-8bfe-d4daf774beeb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create baseline using the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5850b11d-0b29-4d5b-a352-370d4b0a688f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.85629697301126\n"
     ]
    }
   ],
   "source": [
    "averagelatitude = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averagelatitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd6a731-6c92-4d25-8392-2f5a88e48885",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08663353267833755\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def squaredError(label, prediction):\n",
    "    sqrError = (label-prediction)*(label-prediction)\n",
    "    return sqrError\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    sqrSum = labelsAndPreds.map(lambda s: squaredError(s[0],s[1])).sum()\n",
    "    return math.sqrt(sqrSum/labelsAndPreds.count())\n",
    "\n",
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagelongitude))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagelongitude))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed6c6f0-fd0f-475a-ab65-e88e743030ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apply linear regression with weights version one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3f1361-d596-41e8-ad2a-b7d348896e19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# Values to use when training the linear regression model\n",
    "numIters = 500  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee6015c-e20b-4e05-b4c5-564b61f649d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/mllib/regression.py:367: FutureWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\", FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08129066491587728,-0.0431210922907528,0.024292820696472303,0.024894161025153475,-0.005776763546226742,0.0023650160069275676] 38.08515647193387\n"
     ]
    }
   ],
   "source": [
    "firstModel = LinearRegressionWithSGD.train(parsedTrainData,numIters,alpha,miniBatchFrac,initialWeights=None,regParam=reg,regType=regType,intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR1 = firstModel.weights\n",
    "interceptLR1 = firstModel.intercept\n",
    "print(weightsLR1, interceptLR1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c9f3f48-fd0d-4301-9550-b5539f4fe862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.932253638, [-0.11729077624163668,0.151743427286575,1.4282856857085708,-0.2740935175684388,1.8662789830397404,1.153593366866704]), LabeledPoint(41.768392247, [-0.11729077624163668,0.17854407821960258,-0.7001400420140046,-0.2740935175684388,1.8525707481279268,-0.4050394488109772]), LabeledPoint(41.807530373, [-0.11729077624163668,-1.0858130395007504,1.4282856857085708,-0.2740935175684388,1.8434319248533844,1.153593366866704]), LabeledPoint(41.876914678, [-0.11729077624163668,-1.1185849931706981,-0.7001400420140046,4.487302444763297,1.8251542783042995,-0.4050394488109772]), LabeledPoint(41.852951173, [-0.11729077624163668,0.8242539587043363,-0.7001400420140046,-0.2740935175684388,-0.6697444756457859,-0.4050394488109772])]\n38.08889945869792\n38.032431318459984\n38.14239622737014\n38.207054277114665\n38.019158421747335\n"
     ]
    }
   ],
   "source": [
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = firstModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0ba8ff-e42e-4e04-b5ce-9e0d69ab26e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08663353267833755\n3.792155122297883\n"
     ]
    }
   ],
   "source": [
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label,firstModel.predict(lp.features)))\n",
    "rmseValLR1 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseValLR1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3607335-b7fa-4104-bc43-55ed7fe4c162",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apply linear regression with weights 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb5092f-4861-40bf-bae8-516dca18a036",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numIters = 1000  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 0.3  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87314f4e-eaa5-4025-acb1-726c460813d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04444208382874957,-0.013566012936012102,0.033738936794535414,0.029195007806519672,-0.09553023188832116,-0.01999563353405005] 38.05148829900105\n"
     ]
    }
   ],
   "source": [
    "secondModel = LinearRegressionWithSGD.train(parsedTrainData,numIters,alpha,miniBatchFrac,initialWeights=None,regParam=reg,regType=regType,intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR2 = secondModel.weights\n",
    "interceptLR2 = secondModel.intercept\n",
    "print(weightsLR2, interceptLR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43804139-70fc-4578-b73b-d939e4627e49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08663353267833755\n3.821409973912287\n"
     ]
    }
   ],
   "source": [
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label,secondModel.predict(lp.features)))\n",
    "rmseValLR2 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseValLR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c125c1ec-1216-4879-8ff5-9ae5567c148f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Version One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2145917a-c256-4438-ac93-eb8799c97647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81979d1d-7177-43e8-8df1-a82f8a31405b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.932253638, [-0.11729077624163668,0.151743427286575,1.4282856857085708,-0.2740935175684388,1.8662789830397404,1.153593366866704]), LabeledPoint(41.768392247, [-0.11729077624163668,0.17854407821960258,-0.7001400420140046,-0.2740935175684388,1.8525707481279268,-0.4050394488109772]), LabeledPoint(41.807530373, [-0.11729077624163668,-1.0858130395007504,1.4282856857085708,-0.2740935175684388,1.8434319248533844,1.153593366866704]), LabeledPoint(41.876914678, [-0.11729077624163668,-1.1185849931706981,-0.7001400420140046,4.487302444763297,1.8251542783042995,-0.4050394488109772]), LabeledPoint(41.852951173, [-0.11729077624163668,0.8242539587043363,-0.7001400420140046,-0.2740935175684388,-0.6697444756457859,-0.4050394488109772])]\n41.81057953937727\n41.86332844764969\n41.81603843466649\n41.88039262902279\n41.86552153138955\n"
     ]
    }
   ],
   "source": [
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6daa57-3abf-4bd5-a4cd-c026719687d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08663353267833755\n0.07960741793617669\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a91e76-099a-4554-99b0-a8f43a314308",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Version Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc811b0c-ff89-45e5-a873-be0e4193e986",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=6, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90052f2-3452-42b2-a114-8093a1b495f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.932253638, [-0.11729077624163668,0.151743427286575,1.4282856857085708,-0.2740935175684388,1.8662789830397404,1.153593366866704]), LabeledPoint(41.768392247, [-0.11729077624163668,0.17854407821960258,-0.7001400420140046,-0.2740935175684388,1.8525707481279268,-0.4050394488109772]), LabeledPoint(41.807530373, [-0.11729077624163668,-1.0858130395007504,1.4282856857085708,-0.2740935175684388,1.8434319248533844,1.153593366866704]), LabeledPoint(41.876914678, [-0.11729077624163668,-1.1185849931706981,-0.7001400420140046,4.487302444763297,1.8251542783042995,-0.4050394488109772]), LabeledPoint(41.852951173, [-0.11729077624163668,0.8242539587043363,-0.7001400420140046,-0.2740935175684388,-0.6697444756457859,-0.4050394488109772])]\n41.84329119906854\n41.86504485416342\n41.85140157775568\n41.86297343474248\n41.86265478823708\n"
     ]
    }
   ],
   "source": [
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44be81f7-4051-450a-a647-9c754c8a9594",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08663353267833755\n0.07740030933230682\n"
     ]
    }
   ],
   "source": [
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Traffic Crash Analysis (2)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
