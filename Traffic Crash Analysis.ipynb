{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf2e8825-b1d2-4c6d-81ee-75c05f88d00c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Traffic Crash Analysis\n",
    "\n",
    "### Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3229176b-3990-44da-9583-71919c5cd60e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Make the GET request\n",
    "resp = requests.get('https://data.cityofchicago.org/resource/85ca-t3if.json?$query=SELECT%20crash_record_id%2C%20crash_date_est_i%2C%20crash_date%2C%20posted_speed_limit%2C%20traffic_control_device%2C%20device_condition%2C%20weather_condition%2C%20lighting_condition%2C%20first_crash_type%2C%20trafficway_type%2C%20lane_cnt%2C%20alignment%2C%20roadway_surface_cond%2C%20road_defect%2C%20report_type%2C%20crash_type%2C%20intersection_related_i%2C%20private_property_i%2C%20hit_and_run_i%2C%20damage%2C%20date_police_notified%2C%20prim_contributory_cause%2C%20sec_contributory_cause%2C%20street_no%2C%20street_direction%2C%20street_name%2C%20beat_of_occurrence%2C%20photos_taken_i%2C%20statements_taken_i%2C%20dooring_i%2C%20work_zone_i%2C%20work_zone_type%2C%20workers_present_i%2C%20num_units%2C%20most_severe_injury%2C%20injuries_total%2C%20injuries_fatal%2C%20injuries_incapacitating%2C%20injuries_non_incapacitating%2C%20injuries_reported_not_evident%2C%20injuries_no_indication%2C%20injuries_unknown%2C%20crash_hour%2C%20crash_day_of_week%2C%20crash_month%2C%20latitude%2C%20longitude%2C%20location%20ORDER%20BY%20crash_date%20DESC%2C%20crash_record_id%20ASC')\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SENG550\").getOrCreate()\n",
    "\n",
    "# Create a Spark DataFrame from the response text\n",
    "df2 = spark.read.json(spark.sparkContext.parallelize([resp.text]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb8512f-928f-45de-8a07-38be8594f2e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+----------------+-----------------+----------+-----------+--------------------+--------------------+-------------+--------------------+--------------------+---------+--------------------+-------------+--------------+-----------------------+----------------------+---------------------------+-----------------------------+--------------+----------------+----------------------+------------+--------------------+--------------------+-------------+--------------------+---------+--------------+------------------+-----------------------+------------------+--------------------+-----------+--------------------+----------------------+------------------+----------------+--------------------+---------+----------------------+--------------------+-----------------+-----------+--------------+-----------------+\n|         alignment|beat_of_occurrence|          crash_date|crash_date_est_i|crash_day_of_week|crash_hour|crash_month|     crash_record_id|          crash_type|       damage|date_police_notified|    device_condition|dooring_i|    first_crash_type|hit_and_run_i|injuries_fatal|injuries_incapacitating|injuries_no_indication|injuries_non_incapacitating|injuries_reported_not_evident|injuries_total|injuries_unknown|intersection_related_i|    latitude|  lighting_condition|            location|    longitude|  most_severe_injury|num_units|photos_taken_i|posted_speed_limit|prim_contributory_cause|private_property_i|         report_type|road_defect|roadway_surface_cond|sec_contributory_cause|statements_taken_i|street_direction|         street_name|street_no|traffic_control_device|     trafficway_type|weather_condition|work_zone_i|work_zone_type|workers_present_i|\n+------------------+------------------+--------------------+----------------+-----------------+----------+-----------+--------------------+--------------------+-------------+--------------------+--------------------+---------+--------------------+-------------+--------------+-----------------------+----------------------+---------------------------+-----------------------------+--------------+----------------+----------------------+------------+--------------------+--------------------+-------------+--------------------+---------+--------------+------------------+-----------------------+------------------+--------------------+-----------+--------------------+----------------------+------------------+----------------+--------------------+---------+----------------------+--------------------+-----------------+-----------+--------------+-----------------+\n|STRAIGHT AND LEVEL|               621|2023-11-22T01:40:...|               N|                4|         1|         11|6d3a24a1f2031e657...|INJURY AND / OR T...|  OVER $1,500|2023-11-22T01:40:...|         NO CONTROLS|     null|PARKED MOTOR VEHICLE|         null|             0|                      0|                     1|                          0|                            0|             0|               0|                  null| 41.75633481|DARKNESS, LIGHTED...|{[-87.63634482233...|-87.636344822|NO INDICATION OF ...|        2|          null|                30|   IMPROPER OVERTAKI...|              null|            ON SCENE| NO DEFECTS|                 DRY|        NOT APPLICABLE|              null|               W|             76TH ST|      448|           NO CONTROLS|DIVIDED - W/MEDIA...|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|               231|2023-11-22T00:50:...|            null|                4|         0|         11|05d52d810515672d4...|NO INJURY / DRIVE...|  OVER $1,500|2023-11-22T00:51:...|         NO CONTROLS|     null|PARKED MOTOR VEHICLE|            Y|             0|                      0|                     1|                          0|                            0|             0|               0|                  null|41.801450146|DARKNESS, LIGHTED...|{[-87.62086597374...|-87.620865974|NO INDICATION OF ...|        2|          null|                30|    UNABLE TO DETERMINE|              null|            ON SCENE| NO DEFECTS|                 DRY|   UNABLE TO DETERMINE|              null|               S|         INDIANA AVE|     5119|           NO CONTROLS|             ONE-WAY|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|              2525|2023-11-22T00:20:...|            null|                4|         0|         11|f2f8dcea80a5906f3...|NO INJURY / DRIVE...|$501 - $1,500|2023-11-22T01:00:...|         NO CONTROLS|     null|               ANGLE|         null|             0|                      0|                     3|                          0|                            0|             0|               0|                  null|41.923994137|            DARKNESS|{[-87.72201425697...|-87.722014257|NO INDICATION OF ...|        2|          null|                30|    UNABLE TO DETERMINE|              null|NOT ON SCENE (DES...|    UNKNOWN|                 DRY|   UNABLE TO DETERMINE|              null|               N|          HAMLIN AVE|     2342|           NO CONTROLS|             ONE-WAY|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|              1113|2023-11-22T00:01:...|            null|                4|         0|         11|66c517d83cb470b10...|INJURY AND / OR T...|  OVER $1,500|2023-11-22T00:11:...|FUNCTIONING PROPERLY|     null|            REAR END|            Y|             0|                      0|                     1|                          1|                            3|             4|               0|                  null|41.883079908|DARKNESS, LIGHTED...|{[-87.74012911451...|-87.740129115|NONINCAPACITATING...|        2|          null|                30|   FAILING TO REDUCE...|              null|            ON SCENE| NO DEFECTS|                 DRY|  FOLLOWING TOO CLO...|              null|               W|        WEST END AVE|     4559|     STOP SIGN/FLASHER|            FOUR WAY|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|               633|2023-11-21T23:50:...|            null|                3|        23|         11|1a3372cbd11efd059...|INJURY AND / OR T...|  OVER $1,500|2023-11-21T23:59:...|         NO CONTROLS|     null|PARKED MOTOR VEHICLE|            Y|             0|                      0|                     1|                          0|                            0|             0|               0|                  null|41.727288038|DARKNESS, LIGHTED...|{[-87.61442881314...|-87.614428813|NO INDICATION OF ...|        4|          null|                30|   FAILING TO REDUCE...|              null|            ON SCENE| NO DEFECTS|                 DRY|  FAILING TO REDUCE...|              null|               S|DR MARTIN LUTHER ...|     9164|           NO CONTROLS|         NOT DIVIDED|            CLEAR|       null|          null|             null|\n+------------------+------------------+--------------------+----------------+-----------------+----------+-----------+--------------------+--------------------+-------------+--------------------+--------------------+---------+--------------------+-------------+--------------+-----------------------+----------------------+---------------------------+-----------------------------+--------------+----------------+----------------------+------------+--------------------+--------------------+-------------+--------------------+---------+--------------+------------------+-----------------------+------------------+--------------------+-----------+--------------------+----------------------+------------------+----------------+--------------------+---------+----------------------+--------------------+-----------------+-----------+--------------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc35b820-a855-491c-b724-89015c9c00ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extract Weather Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad552b8-4a47-4789-a01e-3e657c9b1ede",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Records from Weather condition:\n['CLEAR', 'CLEAR', 'CLEAR', 'CLEAR', 'CLEAR', 'CLEAR', 'CLEAR', 'CLEAR', 'CLEAR', 'CLEAR']\n"
     ]
    }
   ],
   "source": [
    "feature_weather = df2.select(\"Weather_condition\")\n",
    "# Extract the values from the selected column using rdd.map\n",
    "rdd_from_weather = feature_weather.rdd.map(lambda row: row[0])\n",
    "\n",
    "# Display the top 10 records\n",
    "print(\"Top 10 Records from Weather condition:\")\n",
    "print(rdd_from_weather.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f0a5850-92bb-4fe4-8449-8a533b939e7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extract Crash Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570cfba3-73ed-45e1-9f1b-3ecbe532815f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Records from Crash Type:\n['INJURY AND / OR TOW DUE TO CRASH', 'NO INJURY / DRIVE AWAY', 'NO INJURY / DRIVE AWAY', 'INJURY AND / OR TOW DUE TO CRASH', 'INJURY AND / OR TOW DUE TO CRASH', 'NO INJURY / DRIVE AWAY', 'INJURY AND / OR TOW DUE TO CRASH', 'NO INJURY / DRIVE AWAY', 'INJURY AND / OR TOW DUE TO CRASH', 'NO INJURY / DRIVE AWAY']\n"
     ]
    }
   ],
   "source": [
    "feature_crashType = df2.select(\"Crash_type\")\n",
    "# Extract the values from the selected column using rdd.map\n",
    "rdd_from_crashType = feature_crashType.rdd.map(lambda row: row[0])\n",
    "\n",
    "# Display the top 10 records\n",
    "print(\"Top 10 Records from Crash Type:\")\n",
    "print(rdd_from_crashType.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60d4ebcf-6f09-4954-a7d9-fed354d71f40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extract Lighting Condition and Time of Year Crash Occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a760d2a-2c76-4244-bfca-4e1e1e33d44a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Records from Crash Date and Time:\n['2023-11-22T01:40:00.000', '2023-11-22T00:50:00.000', '2023-11-22T00:20:00.000', '2023-11-22T00:01:00.000', '2023-11-21T23:50:00.000', '2023-11-21T23:46:00.000', '2023-11-21T22:55:00.000', '2023-11-21T21:50:00.000', '2023-11-21T21:48:00.000', '2023-11-21T21:45:00.000']\nTop 10 Records from Lighting Condition:\n['DARKNESS, LIGHTED ROAD', 'DARKNESS, LIGHTED ROAD', 'DARKNESS', 'DARKNESS, LIGHTED ROAD', 'DARKNESS, LIGHTED ROAD', 'DARKNESS, LIGHTED ROAD', 'DARKNESS, LIGHTED ROAD', 'DARKNESS', 'DARKNESS', 'DARKNESS']\n"
     ]
    }
   ],
   "source": [
    "feature_crashDate = df2.select(\"Crash_date\")\n",
    "# Extract the values from the selected column using rdd.map\n",
    "rdd_from_crashDate = feature_crashDate.rdd.map(lambda row: row[0])\n",
    "\n",
    "feature_lighting = df2.select(\"Lighting_condition\")\n",
    "rdd_from_lighting = feature_lighting.rdd.map(lambda row: row[0]) \n",
    "\n",
    "# Display the top 10 records\n",
    "print(\"First 10 Records from Crash Date and Time:\")\n",
    "print(rdd_from_crashDate.take(10))irst\n",
    "\n",
    "print(\"First 10 Records from Lighting Condition:\")\n",
    "print(rdd_from_lighting.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "085c3c0c-0607-41a3-be18-090febb9513f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extract Longitude and Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83f3caf-2837-4470-8a0a-02eca010d691",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Records from Longitude:\n['-87.636344822', '-87.620865974', '-87.722014257', '-87.740129115', '-87.614428813', '-87.596070684', '-87.566154615', '-87.719071202', '-87.662862407', '-87.717587476']\nFirst 10 Records from Latitude:\n['41.75633481', '41.801450146', '41.923994137', '41.883079908', '41.727288038', '41.736662284', '41.755271379', '41.745775042', '41.772320473', '41.946613704']\n"
     ]
    }
   ],
   "source": [
    "feature_longitude = df2.select(\"Longitude\")\n",
    "# Extract the values from the selected column using rdd.map\n",
    "rdd_from_longitude = feature_longitude.rdd.map(lambda row: row[0])\n",
    "\n",
    "feature_latitude = df2.select(\"Latitude\")\n",
    "rdd_from_latitude = feature_latitude.rdd.map(lambda row: row[0]) \n",
    "\n",
    "# Display the first 10 records\n",
    "print(\"First 10 Records from Longitude:\")\n",
    "print(rdd_from_longitude.take(10))\n",
    "\n",
    "print(\"First 10 Records from Latitude:\")\n",
    "print(rdd_from_latitude.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9fad5a2-1613-4cec-ae8e-1d128510cfbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create RDD of wanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d16fd3f-6dba-4801-8d28-537a47d1759f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+--------------------+--------------------+------------+-------------+\n|          Crash_type|num_units|Weather_condition|          Crash_date|  Most_severe_injury|    Latitude|    Longitude|\n+--------------------+---------+-----------------+--------------------+--------------------+------------+-------------+\n|INJURY AND / OR T...|        2|            CLEAR|2023-11-23T00:08:...|REPORTED, NOT EVI...|41.757761477|-87.682965832|\n+--------------------+---------+-----------------+--------------------+--------------------+------------+-------------+\nonly showing top 1 row\n\n[['INJURY AND / OR TOW DUE TO CRASH', '2', 'CLEAR', '2023-11-23T00:08:00.000', 'REPORTED, NOT EVIDENT', '41.757761477', '-87.682965832']]\n"
     ]
    }
   ],
   "source": [
    "wanted_columns = df2.select(\"Crash_type\",\"num_units\",\"Weather_condition\",\"Crash_date\",\"Most_severe_injury\",\"Latitude\", \"Longitude\")\n",
    "wanted_columns.show(1)\n",
    "rdd_of_features = wanted_columns.rdd.map(lambda row:[row[0],row[1],row[2],row[3],row[4],row[5],row[6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04cf460a-c0c5-4d25-b736-79f06b69fc19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Remove all rows where the content of one of the fields is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8e8d5e-837d-4d7f-b335-4099e24b27db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n933\n"
     ]
    }
   ],
   "source": [
    "print(rdd_of_features.count())\n",
    "#row[0] = Crash_type, row[2] = Weather_condition,  row[4]= Most_severe_injury\n",
    "cleaned_data_rdd = rdd_of_features.filter(lambda row: row[0]!=\"UNKNOWN\"  and row[2]!=\"UNKNOWN\"  and row[4]!=\"UNKNOWN\")\n",
    "print(cleaned_data_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db499f2b-8f48-4a7d-8fb7-b399b739179d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apply linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6a14eb-205d-441d-884b-f6aba0d71546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1870660999485449>:8\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m feature_columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_3\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_4\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m      7\u001B[0m assembler \u001B[38;5;241m=\u001B[39m VectorAssembler(inputCols\u001B[38;5;241m=\u001B[39mfeature_columns, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 8\u001B[0m cleaned_data_df \u001B[38;5;241m=\u001B[39m assembler\u001B[38;5;241m.\u001B[39mtransform(cleaned_data_df)\n",
       "\u001B[1;32m     10\u001B[0m lr \u001B[38;5;241m=\u001B[39m LinearRegression(featuresCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLongitude\u001B[39m\u001B[38;5;124m\"\u001B[39m,maxIter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, regParam\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, elasticNetParam\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#training = cleaned_data_rdd.take(500).toDF([\"Crash_type\",\"num_units\",\"Weather_condition\",\"Crash_date\",\"Most_severe_injury\",\"Latitude\", \"Longitude\"])\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Fit the model\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:262\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_transform(dataset)\n",
       "\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be a param map but got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:400\u001B[0m, in \u001B[0;36mJavaTransformer._transform\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n",
       "\u001B[0;32m--> 400\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, dataset\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: Data type string of column _1 is not supported.\n",
       "Data type string of column _2 is not supported.\n",
       "Data type string of column _3 is not supported.\n",
       "Data type string of column _4 is not supported."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-1870660999485449>:8\u001B[0m\n\u001B[1;32m      6\u001B[0m feature_columns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_3\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_4\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      7\u001B[0m assembler \u001B[38;5;241m=\u001B[39m VectorAssembler(inputCols\u001B[38;5;241m=\u001B[39mfeature_columns, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m cleaned_data_df \u001B[38;5;241m=\u001B[39m assembler\u001B[38;5;241m.\u001B[39mtransform(cleaned_data_df)\n\u001B[1;32m     10\u001B[0m lr \u001B[38;5;241m=\u001B[39m LinearRegression(featuresCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m\"\u001B[39m, labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLongitude\u001B[39m\u001B[38;5;124m\"\u001B[39m,maxIter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, regParam\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, elasticNetParam\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#training = cleaned_data_rdd.take(500).toDF([\"Crash_type\",\"num_units\",\"Weather_condition\",\"Crash_date\",\"Most_severe_injury\",\"Latitude\", \"Longitude\"])\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Fit the model\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:262\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_transform(dataset)\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be a param map but got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:400\u001B[0m, in \u001B[0;36mJavaTransformer._transform\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 400\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, dataset\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Data type string of column _1 is not supported.\nData type string of column _2 is not supported.\nData type string of column _3 is not supported.\nData type string of column _4 is not supported.",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: Data type string of column _1 is not supported.\nData type string of column _2 is not supported.\nData type string of column _3 is not supported.\nData type string of column _4 is not supported.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Traffic Crash Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
