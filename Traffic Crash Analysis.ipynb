{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2e8825-b1d2-4c6d-81ee-75c05f88d00c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Traffic Crash Analysis\n",
    "\n",
    "### Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3229176b-3990-44da-9583-71919c5cd60e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Make the GET request\n",
    "resp = requests.get('https://data.cityofchicago.org/resource/85ca-t3if.json?$query=SELECT%20crash_record_id%2C%20crash_date_est_i%2C%20crash_date%2C%20posted_speed_limit%2C%20traffic_control_device%2C%20device_condition%2C%20weather_condition%2C%20lighting_condition%2C%20first_crash_type%2C%20trafficway_type%2C%20lane_cnt%2C%20alignment%2C%20roadway_surface_cond%2C%20road_defect%2C%20report_type%2C%20crash_type%2C%20intersection_related_i%2C%20private_property_i%2C%20hit_and_run_i%2C%20damage%2C%20date_police_notified%2C%20prim_contributory_cause%2C%20sec_contributory_cause%2C%20street_no%2C%20street_direction%2C%20street_name%2C%20beat_of_occurrence%2C%20photos_taken_i%2C%20statements_taken_i%2C%20dooring_i%2C%20work_zone_i%2C%20work_zone_type%2C%20workers_present_i%2C%20num_units%2C%20most_severe_injury%2C%20injuries_total%2C%20injuries_fatal%2C%20injuries_incapacitating%2C%20injuries_non_incapacitating%2C%20injuries_reported_not_evident%2C%20injuries_no_indication%2C%20injuries_unknown%2C%20crash_hour%2C%20crash_day_of_week%2C%20crash_month%2C%20latitude%2C%20longitude%2C%20location%20ORDER%20BY%20crash_date%20DESC%2C%20crash_record_id%20ASC')\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SENG550\").getOrCreate()\n",
    "#df2 = spark.read.csv(\"/FileStore/tables/Traffic_Crashes___Crashes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a Spark DataFrame from the response text\n",
    "df2 = spark.read.json(spark.sparkContext.parallelize([resp.text]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb8512f-928f-45de-8a07-38be8594f2e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+----------------+-----------------+----------+-----------+--------------------+--------------------+-----------+--------------------+--------------------+---------+--------------------+-------------+--------------+-----------------------+----------------------+---------------------------+-----------------------------+--------------+----------------+----------------------+------------+--------------------+--------------------+-------------+--------------------+---------+--------------+------------------+-----------------------+------------------+-----------+------------+--------------------+----------------------+------------------+----------------+--------------+---------+----------------------+--------------------+-----------------+-----------+--------------+-----------------+\n|         alignment|beat_of_occurrence|          crash_date|crash_date_est_i|crash_day_of_week|crash_hour|crash_month|     crash_record_id|          crash_type|     damage|date_police_notified|    device_condition|dooring_i|    first_crash_type|hit_and_run_i|injuries_fatal|injuries_incapacitating|injuries_no_indication|injuries_non_incapacitating|injuries_reported_not_evident|injuries_total|injuries_unknown|intersection_related_i|    latitude|  lighting_condition|            location|    longitude|  most_severe_injury|num_units|photos_taken_i|posted_speed_limit|prim_contributory_cause|private_property_i|report_type| road_defect|roadway_surface_cond|sec_contributory_cause|statements_taken_i|street_direction|   street_name|street_no|traffic_control_device|     trafficway_type|weather_condition|work_zone_i|work_zone_type|workers_present_i|\n+------------------+------------------+--------------------+----------------+-----------------+----------+-----------+--------------------+--------------------+-----------+--------------------+--------------------+---------+--------------------+-------------+--------------+-----------------------+----------------------+---------------------------+-----------------------------+--------------+----------------+----------------------+------------+--------------------+--------------------+-------------+--------------------+---------+--------------+------------------+-----------------------+------------------+-----------+------------+--------------------+----------------------+------------------+----------------+--------------+---------+----------------------+--------------------+-----------------+-----------+--------------+-----------------+\n|STRAIGHT AND LEVEL|              1831|2023-12-14T05:02:...|            null|                5|         5|         12|f7a08044b26987610...|NO INJURY / DRIVE...|OVER $1,500|2023-12-14T05:07:...|             UNKNOWN|     null|SIDESWIPE SAME DI...|         null|             0|                      0|                     2|                          0|                            0|             0|               0|                  null|41.892465831|                DAWN|{[-87.63654818132...|-87.636548181|NO INDICATION OF ...|        2|          null|                30|    UNABLE TO DETERMINE|              null|   ON SCENE|     UNKNOWN|                 DRY|   UNABLE TO DETERMINE|              null|               W|       OHIO ST|      320|        TRAFFIC SIGNAL|             ONE-WAY|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|               834|2023-12-14T02:27:...|            null|                5|         2|         12|67ad96afb2f077a56...|NO INJURY / DRIVE...|OVER $1,500|2023-12-14T02:39:...|         NO CONTROLS|     null|PARKED MOTOR VEHICLE|            Y|             0|                      0|                     1|                          0|                            0|             0|               0|                  null|41.743760507|DARKNESS, LIGHTED...|{[-87.73109557742...|-87.731095577|NO INDICATION OF ...|        2|          null|                30|    UNABLE TO DETERMINE|              null|   ON SCENE|WORN SURFACE|             UNKNOWN|  DRIVING SKILLS/KN...|              null|               S|   KOSTNER AVE|     8169|           NO CONTROLS|         NOT DIVIDED|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|              1624|2023-12-14T01:00:...|            null|                5|         1|         12|c8fa0f3dbe468690f...|INJURY AND / OR T...|OVER $1,500|2023-12-14T01:01:...|FUNCTIONING PROPERLY|     null|             TURNING|         null|             0|                      0|                     4|                          0|                            0|             0|               0|                     Y|41.957818587|DARKNESS, LIGHTED...|{[-87.75714941959...| -87.75714942|NO INDICATION OF ...|        2|          null|                25|   FAILING TO YIELD ...|              null|   ON SCENE|  NO DEFECTS|                 DRY|   UNABLE TO DETERMINE|              null|               N|   LARAMIE AVE|     4227|     STOP SIGN/FLASHER|            FOUR WAY|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|               824|2023-12-14T00:18:...|            null|                5|         0|         12|9def7081563de255e...|INJURY AND / OR T...|OVER $1,500|2023-12-14T00:19:...|FUNCTIONING PROPERLY|     null|        FIXED OBJECT|            Y|             0|                      0|                     1|                          0|                            0|             0|               0|                     Y|41.791964031|DARKNESS, LIGHTED...|{[-87.69360308164...|-87.693603082|NO INDICATION OF ...|        1|          null|                30|    UNABLE TO DETERMINE|              null|   ON SCENE|  NO DEFECTS|                 DRY|   UNABLE TO DETERMINE|              null|               S|CALIFORNIA AVE|     5559|     STOP SIGN/FLASHER|         NOT DIVIDED|            CLEAR|       null|          null|             null|\n|STRAIGHT AND LEVEL|               815|2023-12-13T23:32:...|            null|                4|        23|         12|32cba21ecc85df2e6...|INJURY AND / OR T...|OVER $1,500|2023-12-13T23:32:...|FUNCTIONING PROPERLY|     null|             TURNING|         null|             0|                      0|                     1|                          2|                            0|             2|               0|                     Y|41.819052466|DARKNESS, LIGHTED...|{[-87.7241127025,...|-87.724112702|NONINCAPACITATING...|        2|          null|                30|    UNABLE TO DETERMINE|              null|   ON SCENE|  NO DEFECTS|                 DRY|   UNABLE TO DETERMINE|              null|               S|    PULASKI RD|     4000|        TRAFFIC SIGNAL|UNKNOWN INTERSECT...|            CLEAR|       null|          null|             null|\n+------------------+------------------+--------------------+----------------+-----------------+----------+-----------+--------------------+--------------------+-----------+--------------------+--------------------+---------+--------------------+-------------+--------------+-----------------------+----------------------+---------------------------+-----------------------------+--------------+----------------+----------------------+------------+--------------------+--------------------+-------------+--------------------+---------+--------------+------------------+-----------------------+------------------+-----------+------------+--------------------+----------------------+------------------+----------------+--------------+---------+----------------------+--------------------+-----------------+-----------+--------------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fad5a2-1613-4cec-ae8e-1d128510cfbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create RDD of wanted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d16fd3f-6dba-4801-8d28-537a47d1759f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+--------------------+--------------------+-------------+------------+\n|          Crash_type|num_units|Weather_condition|          Crash_date|  Most_severe_injury|    Longitude|    Latitude|\n+--------------------+---------+-----------------+--------------------+--------------------+-------------+------------+\n|NO INJURY / DRIVE...|        2|            CLEAR|2023-12-14T05:02:...|NO INDICATION OF ...|-87.636548181|41.892465831|\n+--------------------+---------+-----------------+--------------------+--------------------+-------------+------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "wanted_columns = df2.select(\"Crash_type\",\"num_units\",\"Weather_condition\",\"Crash_date\",\"Most_severe_injury\",\"Longitude\",\"Latitude\")\n",
    "wanted_columns.show(1)\n",
    "rdd_of_features = wanted_columns.rdd.map(lambda row:[row[0],row[1],row[2],row[3],row[4],row[5],row[6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cf460a-c0c5-4d25-b736-79f06b69fc19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Remove all rows where the content of one of the fields is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8e8d5e-837d-4d7f-b335-4099e24b27db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n936\n"
     ]
    }
   ],
   "source": [
    "print(rdd_of_features.count())\n",
    "#row[0] = Crash_type, row[2] = Weather_condition,  row[4]= Most_severe_injury\n",
    "cleaned_data_rdd = rdd_of_features.filter(lambda row: row[0]!=\"UNKNOWN\"  and row[2]!=\"UNKNOWN\"  and row[4]!=\"UNKNOWN\" and row[5] != None and row[6] != None and row[0] != None and row[1] != None and row[2] != None and row[3] != None and row[4] != None and row[5]!= 0 and row[6]!=0)\n",
    "print(cleaned_data_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db499f2b-8f48-4a7d-8fb7-b399b739179d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Dataframe from RDD and get it ready for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6a14eb-205d-441d-884b-f6aba0d71546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_1', 'string'), ('_2', 'string'), ('_3', 'string'), ('_4', 'string'), ('_5', 'string'), ('_6', 'string'), ('_7', 'string')]\n[('_1', 'string'), ('_2', 'double'), ('_3', 'string'), ('_4', 'string'), ('_5', 'string'), ('_6', 'double'), ('_7', 'double')]\n+--------------------+---+-----+--------------------+--------------------+-------------+------------+--------+--------+--------+--------+\n|                  _1| _2|   _3|                  _4|                  _5|           _6|          _7|_1_index|_3_index|_4_index|_5_index|\n+--------------------+---+-----+--------------------+--------------------+-------------+------------+--------+--------+--------+--------+\n|NO INJURY / DRIVE...|2.0|CLEAR|2023-12-14T05:02:...|NO INDICATION OF ...|-87.636548181|41.892465831|     0.0|     0.0|   688.0|     0.0|\n|NO INJURY / DRIVE...|2.0|CLEAR|2023-12-14T02:27:...|NO INDICATION OF ...|-87.731095577|41.743760507|     0.0|     0.0|   687.0|     0.0|\n|INJURY AND / OR T...|2.0|CLEAR|2023-12-14T01:00:...|NO INDICATION OF ...| -87.75714942|41.957818587|     1.0|     0.0|   686.0|     0.0|\n|INJURY AND / OR T...|1.0|CLEAR|2023-12-14T00:18:...|NO INDICATION OF ...|-87.693603082|41.791964031|     1.0|     0.0|   685.0|     0.0|\n|INJURY AND / OR T...|2.0|CLEAR|2023-12-13T23:32:...|NONINCAPACITATING...|-87.724112702|41.819052466|     1.0|     0.0|   684.0|     1.0|\n+--------------------+---+-----+--------------------+--------------------+-------------+------------+--------+--------+--------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cleaned_data_df = spark.createDataFrame(cleaned_data_rdd)\n",
    "\n",
    "#_1 = Crash_type, _2 = numUnits, _3 = weather, _4 = time, _5 = injury severity, _6 = longitude, _7 = latitude\n",
    "print(cleaned_data_df.dtypes)\n",
    "numeric_cols = [\"_2\", \"_6\", \"_7\"]\n",
    "for col_name in numeric_cols:    \n",
    "    cleaned_data_df = cleaned_data_df.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "print(cleaned_data_df.dtypes)\n",
    "\n",
    "string_cols = [\"_1\", \"_3\", \"_4\", \"_5\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(cleaned_data_df) for column in string_cols ]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_df = pipeline.fit(cleaned_data_df).transform(cleaned_data_df)\n",
    "indexed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ef1bef-705a-4479-946d-408128dcb4e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8198d30-38e7-452a-b063-3bb0fec0e10f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Labeled Points\n",
    "\n",
    "Partially taken from lab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c911718f-e575-4df5-9983-4a77ea6dd0f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "indexed_rdd = indexed_df.rdd\n",
    "print(indexed_rdd.take(1)[0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e63b46b-d1c4-4968-aefb-c17b5aa0262d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['41.892465831,2.0,-87.636548181,0.0,0.0,688.0,0.0']\n[2.0,-87.636548181,0.0,0.0,688.0,0.0] 41.892465831\n6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def createRDD(values):\n",
    "    return str(values[6]) +',' +str(values[1]) +',' +str(values[5]) +',' +str(values[7]) +',' +str(values[8]) +',' + str(values[9]) +',' + str(values[10])\n",
    "\n",
    "def parsePoint(line):\n",
    "    label_features = line.split(',')\n",
    "    ret_val = LabeledPoint(label_features[0],label_features[1:])\n",
    "    return ret_val\n",
    "\n",
    "indexed_rdd = indexed_df.rdd\n",
    "nice_rdd = indexed_rdd.map(createRDD)\n",
    "print(nice_rdd.take(1))\n",
    "parsedPoints = nice_rdd.map(parsePoint)\n",
    "firstPoint = parsedPoints.take(1)\n",
    "firstPointFeatures =firstPoint[0].features \n",
    "firstPointLabel = firstPoint[0].label\n",
    "print (firstPointFeatures, firstPointLabel)\n",
    "d = len(firstPointFeatures)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187bc96a-b237-4ef8-9ea2-4442f3f3c240",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Normalize features\n",
    "\n",
    "Taken from lab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5b83d8-b35e-4230-a88b-ad5a8e13f731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.892465831, [-0.1311238427122638,0.8548456297589372,-0.6830707265339998,-0.14982623822743735,1.961756223093018,-0.39717293991120234]), LabeledPoint(41.743760507, [-0.1311238427122638,-0.8151993951560601,-0.6830707265339998,-0.14982623822743735,1.9570815935727632,-0.39717293991120234]), LabeledPoint(41.957818587, [-0.1311238427122638,-1.275403406296264,1.4624131662036641,-0.14982623822743735,1.9524069640525086,-0.39717293991120234]), LabeledPoint(41.791964031, [-2.362613238688235,-0.15294789595655534,1.4624131662036641,-0.14982623822743735,1.947732334532254,-0.39717293991120234]), LabeledPoint(41.819052466, [-0.1311238427122638,-0.6918568585370565,1.4624131662036641,-0.14982623822743735,1.9430577050119993,1.145374245802015])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "def normalizeFeatures2(lp):\n",
    "    normalizedFeatures = (lp.features - broadcastMean2.value) / broadcastStdev2.value\n",
    "    return LabeledPoint(lp.label, normalizedFeatures)\n",
    "\n",
    "def getNormalizedRDD2(nonNormalizedRDD): \n",
    "    # Compute column summary statistics\n",
    "    summary = Statistics.colStats(nonNormalizedRDD.map(lambda lp: lp.features))\n",
    "    meanList = summary.mean()\n",
    "    stdevList = summary.variance()**0.5  # sqrt of variance to get standard deviation\n",
    "\n",
    "    # Broadcast the mean and standard deviation\n",
    "    global broadcastMean2\n",
    "    broadcastMean2 = sc.broadcast(meanList)\n",
    "    global broadcastStdev2\n",
    "    broadcastStdev2 = sc.broadcast(stdevList)\n",
    "\n",
    "    # Normalize the features\n",
    "    returnRDD = nonNormalizedRDD.map(lambda lp: normalizeFeatures2(lp))\n",
    "    return returnRDD\n",
    "\n",
    "normalizedSamplePoints = getNormalizedRDD2(parsedPoints)\n",
    "print(normalizedSamplePoints.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe5f8f8-fbd0-450c-a6d5-02929501afb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728 208 936\n936\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)\n",
    "print(normalizedSamplePoints.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a4ba87c-f94d-4c2e-8bfe-d4daf774beeb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create baseline using the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5850b11d-0b29-4d5b-a352-370d4b0a688f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.8622154629863\n"
     ]
    }
   ],
   "source": [
    "averagelatitude = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averagelatitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd6a731-6c92-4d25-8392-2f5a88e48885",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08687977088552416\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def squaredError(label, prediction):\n",
    "    sqrError = (label-prediction)*(label-prediction)\n",
    "    return sqrError\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    sqrSum = labelsAndPreds.map(lambda s: squaredError(s[0],s[1])).sum()\n",
    "    return math.sqrt(sqrSum/labelsAndPreds.count())\n",
    "\n",
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagelatitude))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed6c6f0-fd0f-475a-ab65-e88e743030ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apply linear regression with weights version one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3f1361-d596-41e8-ad2a-b7d348896e19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# Values to use when training the linear regression model\n",
    "numIters = 500  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee6015c-e20b-4e05-b4c5-564b61f649d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/mllib/regression.py:367: FutureWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\", FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0278509863158071,-0.05716169131759258,0.04747734105360197,0.006705664752213127,0.010390822985318198,-0.05073197024761918] 38.09290581828401\n"
     ]
    }
   ],
   "source": [
    "firstModel = LinearRegressionWithSGD.train(parsedTrainData,numIters,alpha,miniBatchFrac,initialWeights=None,regParam=reg,regType=regType,intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR1 = firstModel.weights\n",
    "interceptLR1 = firstModel.intercept\n",
    "print(weightsLR1, interceptLR1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c9f3f48-fd0d-4301-9550-b5539f4fe862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.970677586, [-0.1311238427122638,-1.3863222497408927,-0.6830707265339998,-0.14982623822743735,1.9103352983702169,-0.39717293991120234]), LabeledPoint(41.909814337, [-0.1311238427122638,-0.6723953788384704,-0.6830707265339998,-0.14982623822743735,1.8869621507689436,-0.39717293991120234]), LabeledPoint(41.779926518, [-0.1311238427122638,0.9355479429383212,-0.6830707265339998,-0.14982623822743735,1.868263632687925,-0.39717293991120234]), LabeledPoint(41.965042796, [-0.1311238427122638,0.17082292646035496,1.4624131662036641,-0.14982623822743735,1.840215855566397,1.145374245802015]), LabeledPoint(41.92616072, [-0.1311238427122638,0.943755517384239,-0.6830707265339998,-0.14982623822743735,1.8261919670056332,-0.39717293991120234])]\n38.17506266976803\n38.13401053611084\n38.041903483303784\n38.1089304317207\n38.040997165236035\n"
     ]
    }
   ],
   "source": [
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = firstModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0ba8ff-e42e-4e04-b5ce-9e0d69ab26e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08687977088552416\n3.771046491832153\n"
     ]
    }
   ],
   "source": [
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label,firstModel.predict(lp.features)))\n",
    "rmseValLR1 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseValLR1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3607335-b7fa-4104-bc43-55ed7fe4c162",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apply linear regression with weights 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb5092f-4861-40bf-bae8-516dca18a036",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numIters = 1000  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 0.3  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87314f4e-eaa5-4025-acb1-726c460813d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.006107972239543204,0.003027845295892827,-0.013623843798681638,0.017539556086358026,-0.029371773721555257,-0.07878945919138249] 38.054444585276265\n"
     ]
    }
   ],
   "source": [
    "secondModel = LinearRegressionWithSGD.train(parsedTrainData,numIters,alpha,miniBatchFrac,initialWeights=None,regParam=reg,regType=regType,intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR2 = secondModel.weights\n",
    "interceptLR2 = secondModel.intercept\n",
    "print(weightsLR2, interceptLR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43804139-70fc-4578-b73b-d939e4627e49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08687977088552416\n3.8063269090666023\n"
     ]
    }
   ],
   "source": [
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label,secondModel.predict(lp.features)))\n",
    "rmseValLR2 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseValLR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c125c1ec-1216-4879-8ff5-9ae5567c148f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Version One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2145917a-c256-4438-ac93-eb8799c97647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81979d1d-7177-43e8-8df1-a82f8a31405b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.970677586, [-0.1311238427122638,-1.3863222497408927,-0.6830707265339998,-0.14982623822743735,1.9103352983702169,-0.39717293991120234]), LabeledPoint(41.909814337, [-0.1311238427122638,-0.6723953788384704,-0.6830707265339998,-0.14982623822743735,1.8869621507689436,-0.39717293991120234]), LabeledPoint(41.779926518, [-0.1311238427122638,0.9355479429383212,-0.6830707265339998,-0.14982623822743735,1.868263632687925,-0.39717293991120234]), LabeledPoint(41.965042796, [-0.1311238427122638,0.17082292646035496,1.4624131662036641,-0.14982623822743735,1.840215855566397,1.145374245802015]), LabeledPoint(41.92616072, [-0.1311238427122638,0.943755517384239,-0.6830707265339998,-0.14982623822743735,1.8261919670056332,-0.39717293991120234])]\n41.90226857599271\n41.884560886742484\n41.83739111731327\n41.8585301797274\n41.83739111731327\n"
     ]
    }
   ],
   "source": [
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6daa57-3abf-4bd5-a4cd-c026719687d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08687977088552416\n0.07460726627248873\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a91e76-099a-4554-99b0-a8f43a314308",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Version Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc811b0c-ff89-45e5-a873-be0e4193e986",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=6, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90052f2-3452-42b2-a114-8093a1b495f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(41.970677586, [-0.1311238427122638,-1.3863222497408927,-0.6830707265339998,-0.14982623822743735,1.9103352983702169,-0.39717293991120234]), LabeledPoint(41.909814337, [-0.1311238427122638,-0.6723953788384704,-0.6830707265339998,-0.14982623822743735,1.8869621507689436,-0.39717293991120234]), LabeledPoint(41.779926518, [-0.1311238427122638,0.9355479429383212,-0.6830707265339998,-0.14982623822743735,1.868263632687925,-0.39717293991120234]), LabeledPoint(41.965042796, [-0.1311238427122638,0.17082292646035496,1.4624131662036641,-0.14982623822743735,1.840215855566397,1.145374245802015]), LabeledPoint(41.92616072, [-0.1311238427122638,0.943755517384239,-0.6830707265339998,-0.14982623822743735,1.8261919670056332,-0.39717293991120234])]\n41.90698333894528\n41.88715649167684\n41.86147963580981\n41.86669562823262\n41.85418257742056\n"
     ]
    }
   ],
   "source": [
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44be81f7-4051-450a-a647-9c754c8a9594",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08687977088552416\n0.07599848169250893\n"
     ]
    }
   ],
   "source": [
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509b6b8b-d27e-44e4-93ea-58aebec6a8c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6630ca7d-b585-4496-aefe-6813339ec7c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n['87.636548181,2.0,41.892465831,0.0,0.0,688.0,0.0']\n[2.0,41.892465831,0.0,0.0,688.0,0.0] 87.636548181\n6\n"
     ]
    }
   ],
   "source": [
    "indexed_rdd = indexed_df.rdd\n",
    "print(indexed_rdd.take(1)[0][10])\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def createLatitudeRDD(values):\n",
    "    return str(-values[5]) +',' +str(values[1]) +',' +str(values[6]) +',' +str(values[7]) +',' +str(values[8]) +',' + str(values[9]) +',' + str(values[10])\n",
    "\n",
    "\n",
    "indexed_rdd = indexed_df.rdd\n",
    "nice_rdd = indexed_rdd.map(createLatitudeRDD)\n",
    "print(nice_rdd.take(1))\n",
    "parsedPoints = nice_rdd.map(parsePoint)\n",
    "firstPoint = parsedPoints.take(1)\n",
    "firstPointFeatures =firstPoint[0].features \n",
    "firstPointLabel = firstPoint[0].label\n",
    "print (firstPointFeatures, firstPointLabel)\n",
    "d = len(firstPointFeatures)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a063553d-f4ef-478e-94c3-f7a551244505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(87.636548181, [-0.1311238427122638,0.36495917786870435,-0.6830707265339998,-0.14982623822743735,1.961756223093018,-0.39717293991120234]), LabeledPoint(87.731095577, [-0.1311238427122638,-1.3824393254700018,-0.6830707265339998,-0.14982623822743735,1.9570815935727632,-0.39717293991120234]), LabeledPoint(87.75714942, [-0.1311238427122638,1.1329028193717372,1.4624131662036641,-0.14982623822743735,1.9524069640525086,-0.39717293991120234]), LabeledPoint(87.693603082, [-2.362613238688235,-0.816011954697108,1.4624131662036641,-0.14982623822743735,1.947732334532254,-0.39717293991120234]), LabeledPoint(87.724112702, [-0.1311238427122638,-0.49770263325820874,1.4624131662036641,-0.14982623822743735,1.9430577050119993,1.145374245802015])]\n"
     ]
    }
   ],
   "source": [
    "normalizedSamplePoints = getNormalizedRDD2(parsedPoints)\n",
    "print(normalizedSamplePoints.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0977035f-2a29-4e7b-92ec-1ac86fd0e773",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728 208 936\n936\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)\n",
    "print(normalizedSamplePoints.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55762596-2d80-4c80-aa3f-a77ec43737c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.6853364728132\n0.05564986824222757\n"
     ]
    }
   ],
   "source": [
    "averagelongitude = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averagelongitude)\n",
    "\n",
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagelongitude))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagelongitude))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fd987b-ba72-42c2-a1fc-ac4ef25c13bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.056394434792837705,0.10621308583395057,0.14056899647063403,0.01901456479457416,0.01260512920969955,-0.11592060221387676] 79.790504282462\n[LabeledPoint(87.763428944, [-0.1311238427122638,1.2840056514871308,-0.6830707265339998,-0.14982623822743735,1.9103352983702169,-0.39717293991120234]), LabeledPoint(87.723010916, [-0.1311238427122638,0.568817066773592,-0.6830707265339998,-0.14982623822743735,1.8869621507689436,-0.39717293991120234]), LabeledPoint(87.631979326, [-0.1311238427122638,-0.9574617134166541,-0.6830707265339998,-0.14982623822743735,1.868263632687925,-0.39717293991120234]), LabeledPoint(87.675273224, [-0.1311238427122638,1.2177926649727335,1.4624131662036641,-0.14982623822743735,1.840215855566397,1.145374245802015]), LabeledPoint(87.631514665, [-0.1311238427122638,0.7608992627627377,-0.6830707265339998,-0.14982623822743735,1.8261919670056332,-0.39717293991120234])]\n79.89074093232237\n79.81448392424117\n79.65213744791785\n80.00559990110058\n79.83411955099251\n0.05564986824222757\n7.904762417484847\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# Values to use when training the linear regression model\n",
    "numIters = 500  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept\n",
    "firstModel = LinearRegressionWithSGD.train(parsedTrainData,numIters,alpha,miniBatchFrac,initialWeights=None,regParam=reg,regType=regType,intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR1 = firstModel.weights\n",
    "interceptLR1 = firstModel.intercept\n",
    "print(weightsLR1, interceptLR1)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = firstModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label,firstModel.predict(lp.features)))\n",
    "rmseValLR1 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseValLR1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15c1a11-89f2-4d90-87b4-47c218a54ad1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22545739272629092,0.27862001289184357,0.0919751438503133,-0.11200654525536913,-0.11233637451717271,0.08562077260644438] 79.702157524454\n0.05564986824222757\n8.00541149277323\n"
     ]
    }
   ],
   "source": [
    "### Apply linear regression with weights 2\n",
    "numIters = 1000  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 0.3  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept\n",
    "secondModel = LinearRegressionWithSGD.train(parsedTrainData,numIters,alpha,miniBatchFrac,initialWeights=None,regParam=reg,regType=regType,intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR2 = secondModel.weights\n",
    "interceptLR2 = secondModel.intercept\n",
    "print(weightsLR2, interceptLR2)\n",
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label,secondModel.predict(lp.features)))\n",
    "rmseValLR2 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseValLR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48f34cf-a694-443b-824b-0cf317b1ebec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(87.763428944, [-0.1311238427122638,1.2840056514871308,-0.6830707265339998,-0.14982623822743735,1.9103352983702169,-0.39717293991120234]), LabeledPoint(87.723010916, [-0.1311238427122638,0.568817066773592,-0.6830707265339998,-0.14982623822743735,1.8869621507689436,-0.39717293991120234]), LabeledPoint(87.631979326, [-0.1311238427122638,-0.9574617134166541,-0.6830707265339998,-0.14982623822743735,1.868263632687925,-0.39717293991120234]), LabeledPoint(87.675273224, [-0.1311238427122638,1.2177926649727335,1.4624131662036641,-0.14982623822743735,1.840215855566397,1.145374245802015]), LabeledPoint(87.631514665, [-0.1311238427122638,0.7608992627627377,-0.6830707265339998,-0.14982623822743735,1.8261919670056332,-0.39717293991120234])]\n87.71768948936446\n87.70028743780861\n87.66527216400041\n87.7288705813525\n87.70558366006617\n0.05564986824222757\n0.048489467402581914\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n",
    "\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "import numpy as np\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b169ca5d-7c16-4e26-8d29-52a9ac213774",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(87.763428944, [-0.1311238427122638,1.2840056514871308,-0.6830707265339998,-0.14982623822743735,1.9103352983702169,-0.39717293991120234]), LabeledPoint(87.723010916, [-0.1311238427122638,0.568817066773592,-0.6830707265339998,-0.14982623822743735,1.8869621507689436,-0.39717293991120234]), LabeledPoint(87.631979326, [-0.1311238427122638,-0.9574617134166541,-0.6830707265339998,-0.14982623822743735,1.868263632687925,-0.39717293991120234]), LabeledPoint(87.675273224, [-0.1311238427122638,1.2177926649727335,1.4624131662036641,-0.14982623822743735,1.840215855566397,1.145374245802015]), LabeledPoint(87.631514665, [-0.1311238427122638,0.7608992627627377,-0.6830707265339998,-0.14982623822743735,1.8261919670056332,-0.39717293991120234])]\n87.7033866149425\n87.69129302775625\n87.661066250721\n87.72192593876805\n87.70643891353157\n0.05564986824222757\n0.04864750478797434\n"
     ]
    }
   ],
   "source": [
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=6, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef32da7a-2753-4e5d-b9b3-0c43c34789ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Type of Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ee56df-a913-41f7-b686-57d33056403d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n['0.0,2.0,41.892465831,-87.636548181,0.0,688.0,0.0']\n[2.0,41.892465831,-87.636548181,0.0,688.0,0.0] 0.0\n6\n"
     ]
    }
   ],
   "source": [
    "indexed_rdd = indexed_df.rdd\n",
    "print(indexed_rdd.take(1)[0][10])\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def createLatitudeRDD(values):\n",
    "    return str(values[7]) +',' +str(values[1]) +',' +str(values[6]) +',' +str(values[5]) +',' +str(values[8]) +',' + str(values[9]) +',' + str(values[10])\n",
    "\n",
    "\n",
    "indexed_rdd = indexed_df.rdd\n",
    "nice_rdd = indexed_rdd.map(createLatitudeRDD)\n",
    "print(nice_rdd.take(1))\n",
    "parsedPoints = nice_rdd.map(parsePoint)\n",
    "firstPoint = parsedPoints.take(1)\n",
    "firstPointFeatures =firstPoint[0].features \n",
    "firstPointLabel = firstPoint[0].label\n",
    "print (firstPointFeatures, firstPointLabel)\n",
    "d = len(firstPointFeatures)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b27f8d-f865-4341-b823-97c36a269b37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [-0.1311238427122638,0.36495917786870435,0.8548456297589372,-0.14982623822743735,1.961756223093018,-0.39717293991120234]), LabeledPoint(0.0, [-0.1311238427122638,-1.3824393254700018,-0.8151993951560601,-0.14982623822743735,1.9570815935727632,-0.39717293991120234]), LabeledPoint(1.0, [-0.1311238427122638,1.1329028193717372,-1.275403406296264,-0.14982623822743735,1.9524069640525086,-0.39717293991120234]), LabeledPoint(1.0, [-2.362613238688235,-0.816011954697108,-0.15294789595655534,-0.14982623822743735,1.947732334532254,-0.39717293991120234]), LabeledPoint(1.0, [-0.1311238427122638,-0.49770263325820874,-0.6918568585370565,-0.14982623822743735,1.9430577050119993,1.145374245802015])]\n"
     ]
    }
   ],
   "source": [
    "normalizedSamplePoints = getNormalizedRDD2(parsedPoints)\n",
    "print(normalizedSamplePoints.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69ebd41-390f-478e-9bdf-c5d95ce6f212",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728 208 936\n936\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .2] # train/test split\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights,seed)\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "\n",
    "print(nTrain, nVal, nTrain + nVal)\n",
    "print(normalizedSamplePoints.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662bd924-1797-46c3-9ea3-4425aaf55d4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32142857142857173\n0.46174282498022856\n"
     ]
    }
   ],
   "source": [
    "averagetype = (parsedTrainData.map(lambda s: s.label)).mean()\n",
    "print(averagetype)\n",
    "\n",
    "labelsAndPredsTrain = parsedTrainData.map(lambda s: (s.label,averagetype))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda s: (s.label,averagetype))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "print(rmseValBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c731b72-4e10-440b-b90b-177d289217ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/mllib/regression.py:367: FutureWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\", FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02326861464298533,-0.0462066428119244,-0.031714783140954425,-0.0013044930148267296,0.025514687148054598,0.2379615111280032] 0.29206596608031166\n[LabeledPoint(0.0, [-0.1311238427122638,1.2840056514871308,-1.3863222497408927,-0.14982623822743735,1.9103352983702169,-0.39717293991120234]), LabeledPoint(0.0, [-0.1311238427122638,0.568817066773592,-0.6723953788384704,-0.14982623822743735,1.8869621507689436,-0.39717293991120234]), LabeledPoint(0.0, [-0.1311238427122638,-0.9574617134166541,0.9355479429383212,-0.14982623822743735,1.868263632687925,-0.39717293991120234]), LabeledPoint(1.0, [-0.1311238427122638,1.2177926649727335,0.17082292646035496,-0.14982623822743735,1.840215855566397,1.145374245802015]), LabeledPoint(0.0, [-0.1311238427122638,0.7608992627627377,0.943755517384239,-0.14982623822743735,1.8261919670056332,-0.39717293991120234])]\n0.22807739672757266\n0.2378854657667146\n0.2569370236023187\n0.5470301386151788\n0.17620358492270105\n0.46174282498022856\n0.3617677996681902\n"
     ]
    }
   ],
   "source": [
    "# Values to use when training the linear regression model\n",
    "numIters = 500  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept\n",
    "firstModel = LinearRegressionWithSGD.train(parsedTrainData,numIters,alpha,miniBatchFrac,initialWeights=None,regParam=reg,regType=regType,intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR1 = firstModel.weights\n",
    "interceptLR1 = firstModel.intercept\n",
    "print(weightsLR1, interceptLR1)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = firstModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label,firstModel.predict(lp.features)))\n",
    "rmseValLR1 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseValLR1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c85d7a6-8dbb-4b85-92b1-fe6aba4bc02e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02869086208773176,-0.047236840974027576,-0.029373717589844257,-0.006735946202819645,0.028454446578139697,0.23587209999129188] 0.2944926643477481\n0.46174282498022856\n0.3614619243391188\n"
     ]
    }
   ],
   "source": [
    "### Apply linear regression with weights 2\n",
    "numIters = 1000  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 0.3  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept\n",
    "secondModel = LinearRegressionWithSGD.train(parsedTrainData,numIters,alpha,miniBatchFrac,initialWeights=None,regParam=reg,regType=regType,intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR2 = secondModel.weights\n",
    "interceptLR2 = secondModel.intercept\n",
    "print(weightsLR2, interceptLR2)\n",
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label,secondModel.predict(lp.features)))\n",
    "rmseValLR2 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseValLR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db74c75-37f4-4d47-b950-3b652cb95fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [-0.1311238427122638,1.2840056514871308,-1.3863222497408927,-0.14982623822743735,1.9103352983702169,-0.39717293991120234]), LabeledPoint(0.0, [-0.1311238427122638,0.568817066773592,-0.6723953788384704,-0.14982623822743735,1.8869621507689436,-0.39717293991120234]), LabeledPoint(0.0, [-0.1311238427122638,-0.9574617134166541,0.9355479429383212,-0.14982623822743735,1.868263632687925,-0.39717293991120234]), LabeledPoint(1.0, [-0.1311238427122638,1.2177926649727335,0.17082292646035496,-0.14982623822743735,1.840215855566397,1.145374245802015]), LabeledPoint(0.0, [-0.1311238427122638,0.7608992627627377,0.943755517384239,-0.14982623822743735,1.8261919670056332,-0.39717293991120234])]\n0.19892745580501398\n0.18400703140183094\n0.19060899914108412\n1.0\n0.174310666153604\n0.46174282498022856\n0.3440720770909709\n"
     ]
    }
   ],
   "source": [
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=8, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n",
    "\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "import numpy as np\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT1 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6dbacac-5241-439b-a46b-e2e1c7e0f58c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [-0.1311238427122638,1.2840056514871308,-1.3863222497408927,-0.14982623822743735,1.9103352983702169,-0.39717293991120234]), LabeledPoint(0.0, [-0.1311238427122638,0.568817066773592,-0.6723953788384704,-0.14982623822743735,1.8869621507689436,-0.39717293991120234]), LabeledPoint(0.0, [-0.1311238427122638,-0.9574617134166541,0.9355479429383212,-0.14982623822743735,1.868263632687925,-0.39717293991120234]), LabeledPoint(1.0, [-0.1311238427122638,1.2177926649727335,0.17082292646035496,-0.14982623822743735,1.840215855566397,1.145374245802015]), LabeledPoint(0.0, [-0.1311238427122638,0.7608992627627377,0.943755517384239,-0.14982623822743735,1.8261919670056332,-0.39717293991120234])]\n0.2743721663987147\n0.1751034081455845\n0.25154318168259704\n1.0\n0.1363511677254255\n0.46174282498022856\n0.35061579426617395\n"
     ]
    }
   ],
   "source": [
    "thirdModel = RandomForest.trainRegressor(parsedTrainData, categoricalFeaturesInfo={},\n",
    "                                      numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "                                      impurity='variance', maxDepth=6, maxBins=32)\n",
    "samplePoints = parsedValData.take(5)\n",
    "print(samplePoints)\n",
    "for i in range(5):\n",
    "    samplePrediction = thirdModel.predict(samplePoints[i].features)\n",
    "    print(samplePrediction)\n",
    "labels = parsedValData.map(lambda x: x.label).collect()\n",
    "predictions = thirdModel.predict(parsedValData.map(lambda x: x.features)).collect()\n",
    "rmseDT2 = np.sqrt(np.mean((np.array(predictions)-np.array(labels))**2))\n",
    "\n",
    "print(rmseValBase)\n",
    "print(rmseDT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d9e753-0471-46aa-b667-1ac1ff27786d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Next Steps\n",
    "\n",
    "1. Fix normalization slow run time\n",
    "2. apply random forest model 1 for latitude and longitude\n",
    "3. apply linear regression 2 for all others"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Traffic Crash Analysis (2)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
